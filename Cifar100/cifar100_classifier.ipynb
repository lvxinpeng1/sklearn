{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cifar100-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#引包\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from scipy import misc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文件打开函数\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先查看meta，顺便验证unpickle是否有误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta=unpickle(\"./cifar-100-python/meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'fine_label_names', b'coarse_label_names'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'aquatic_mammals',\n",
       " b'fish',\n",
       " b'flowers',\n",
       " b'food_containers',\n",
       " b'fruit_and_vegetables',\n",
       " b'household_electrical_devices',\n",
       " b'household_furniture',\n",
       " b'insects',\n",
       " b'large_carnivores',\n",
       " b'large_man-made_outdoor_things',\n",
       " b'large_natural_outdoor_scenes',\n",
       " b'large_omnivores_and_herbivores',\n",
       " b'medium_mammals',\n",
       " b'non-insect_invertebrates',\n",
       " b'people',\n",
       " b'reptiles',\n",
       " b'small_mammals',\n",
       " b'trees',\n",
       " b'vehicles_1',\n",
       " b'vehicles_2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[b'coarse_label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#精细标签\n",
    "len(meta[b'fine_label_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#粗糙标签\n",
    "len(meta[b'coarse_label_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unpickle没问题之后可以查看训练集和测试集了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'filenames', b'batch_label', b'fine_labels', b'coarse_labels', b'data'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset=unpickle(\"./cifar-100-python/train\")\n",
    "trainset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[b'data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_x=trainset[b'data']\n",
    "n_trainset=len(trainset_x)\n",
    "n_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 15,\n",
       " 4,\n",
       " 14,\n",
       " 1,\n",
       " 5,\n",
       " 18,\n",
       " 3,\n",
       " 10,\n",
       " 11,\n",
       " 5,\n",
       " 17,\n",
       " 2,\n",
       " 9,\n",
       " 10,\n",
       " 5,\n",
       " 18,\n",
       " 8,\n",
       " 16,\n",
       " 10,\n",
       " 16,\n",
       " 17,\n",
       " 2,\n",
       " 5,\n",
       " 17,\n",
       " 6,\n",
       " 12,\n",
       " 17,\n",
       " 8,\n",
       " 12,\n",
       " 18,\n",
       " 9,\n",
       " 17,\n",
       " 16,\n",
       " 11,\n",
       " 5,\n",
       " 19,\n",
       " 14,\n",
       " 7,\n",
       " 6,\n",
       " 15,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 17,\n",
       " 9,\n",
       " 14,\n",
       " 18,\n",
       " 19,\n",
       " 15,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 10,\n",
       " 7,\n",
       " 17,\n",
       " 6,\n",
       " 14,\n",
       " 17,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 14,\n",
       " 4,\n",
       " 10,\n",
       " 15,\n",
       " 4,\n",
       " 2,\n",
       " 16,\n",
       " 11,\n",
       " 19,\n",
       " 16,\n",
       " 7,\n",
       " 19,\n",
       " 14,\n",
       " 10,\n",
       " 8,\n",
       " 5,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 12,\n",
       " 13,\n",
       " 10,\n",
       " 7,\n",
       " 14,\n",
       " 11,\n",
       " 14,\n",
       " 19,\n",
       " 0,\n",
       " 4,\n",
       " 12,\n",
       " 13,\n",
       " 1,\n",
       " 15,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 15,\n",
       " 4,\n",
       " 19,\n",
       " 17,\n",
       " 6,\n",
       " 17,\n",
       " 12,\n",
       " 6,\n",
       " 0,\n",
       " 14,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 6,\n",
       " 10,\n",
       " 12,\n",
       " 1,\n",
       " 14,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 12,\n",
       " 4,\n",
       " 12,\n",
       " 18,\n",
       " 17,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 12,\n",
       " 11,\n",
       " 9,\n",
       " 5,\n",
       " 17,\n",
       " 2,\n",
       " 14,\n",
       " 12,\n",
       " 5,\n",
       " 0,\n",
       " 13,\n",
       " 12,\n",
       " 16,\n",
       " 4,\n",
       " 13,\n",
       " 0,\n",
       " 18,\n",
       " 15,\n",
       " 10,\n",
       " 15,\n",
       " 5,\n",
       " 13,\n",
       " 1,\n",
       " 14,\n",
       " 12,\n",
       " 3,\n",
       " 19,\n",
       " 1,\n",
       " 2,\n",
       " 15,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 12,\n",
       " 16,\n",
       " 0,\n",
       " 11,\n",
       " 12,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 15,\n",
       " 7,\n",
       " 9,\n",
       " 11,\n",
       " 14,\n",
       " 11,\n",
       " 3,\n",
       " 13,\n",
       " 13,\n",
       " 19,\n",
       " 13,\n",
       " 14,\n",
       " 8,\n",
       " 14,\n",
       " 1,\n",
       " 16,\n",
       " 9,\n",
       " 16,\n",
       " 18,\n",
       " 19,\n",
       " 6,\n",
       " 5,\n",
       " 19,\n",
       " 5,\n",
       " 1,\n",
       " 14,\n",
       " 14,\n",
       " 10,\n",
       " 12,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 16,\n",
       " 19,\n",
       " 9,\n",
       " 13,\n",
       " 16,\n",
       " 11,\n",
       " 13,\n",
       " 9,\n",
       " 5,\n",
       " 18,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 11,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 14,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 13,\n",
       " 12,\n",
       " 19,\n",
       " 11,\n",
       " 13,\n",
       " 5,\n",
       " 15,\n",
       " 5,\n",
       " 18,\n",
       " 6,\n",
       " 14,\n",
       " 11,\n",
       " 3,\n",
       " 17,\n",
       " 2,\n",
       " 16,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 18,\n",
       " 18,\n",
       " 4,\n",
       " 14,\n",
       " 10,\n",
       " 10,\n",
       " 19,\n",
       " 17,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 12,\n",
       " 12,\n",
       " 7,\n",
       " 14,\n",
       " 12,\n",
       " 17,\n",
       " 4,\n",
       " 1,\n",
       " 15,\n",
       " 14,\n",
       " 2,\n",
       " 10,\n",
       " 18,\n",
       " 17,\n",
       " 11,\n",
       " 9,\n",
       " 12,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 18,\n",
       " 15,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 19,\n",
       " 16,\n",
       " 16,\n",
       " 9,\n",
       " 16,\n",
       " 12,\n",
       " 13,\n",
       " 18,\n",
       " 4,\n",
       " 10,\n",
       " 13,\n",
       " 8,\n",
       " 19,\n",
       " 4,\n",
       " 15,\n",
       " 11,\n",
       " 3,\n",
       " 14,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 0,\n",
       " 14,\n",
       " 2,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 3,\n",
       " 12,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 16,\n",
       " 5,\n",
       " 8,\n",
       " 15,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 17,\n",
       " 10,\n",
       " 17,\n",
       " 6,\n",
       " 11,\n",
       " 2,\n",
       " 4,\n",
       " 11,\n",
       " 14,\n",
       " 16,\n",
       " 12,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 16,\n",
       " 2,\n",
       " 13,\n",
       " 13,\n",
       " 16,\n",
       " 0,\n",
       " 14,\n",
       " 11,\n",
       " 11,\n",
       " 18,\n",
       " 9,\n",
       " 1,\n",
       " 10,\n",
       " 5,\n",
       " 13,\n",
       " 11,\n",
       " 7,\n",
       " 18,\n",
       " 4,\n",
       " 0,\n",
       " 19,\n",
       " 14,\n",
       " 8,\n",
       " 19,\n",
       " 3,\n",
       " 8,\n",
       " 14,\n",
       " 6,\n",
       " 0,\n",
       " 18,\n",
       " 15,\n",
       " 18,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 16,\n",
       " 17,\n",
       " 3,\n",
       " 15,\n",
       " 0,\n",
       " 14,\n",
       " 4,\n",
       " 5,\n",
       " 18,\n",
       " 5,\n",
       " 17,\n",
       " 7,\n",
       " 18,\n",
       " 6,\n",
       " 8,\n",
       " 13,\n",
       " 11,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 10,\n",
       " 19,\n",
       " 17,\n",
       " 2,\n",
       " 7,\n",
       " 18,\n",
       " 13,\n",
       " 10,\n",
       " 5,\n",
       " 17,\n",
       " 1,\n",
       " 18,\n",
       " 8,\n",
       " 7,\n",
       " 11,\n",
       " 9,\n",
       " 16,\n",
       " 3,\n",
       " 10,\n",
       " 12,\n",
       " 10,\n",
       " 7,\n",
       " 0,\n",
       " 12,\n",
       " 7,\n",
       " 4,\n",
       " 17,\n",
       " 10,\n",
       " 12,\n",
       " 2,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 1,\n",
       " 6,\n",
       " 18,\n",
       " 18,\n",
       " 15,\n",
       " 9,\n",
       " 14,\n",
       " 2,\n",
       " 13,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 12,\n",
       " 10,\n",
       " 16,\n",
       " 6,\n",
       " 18,\n",
       " 2,\n",
       " 10,\n",
       " 3,\n",
       " 3,\n",
       " 15,\n",
       " 17,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 11,\n",
       " 19,\n",
       " 5,\n",
       " 10,\n",
       " 19,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 11,\n",
       " 0,\n",
       " 17,\n",
       " 0,\n",
       " 19,\n",
       " 14,\n",
       " 8,\n",
       " 7,\n",
       " 14,\n",
       " 10,\n",
       " 19,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 15,\n",
       " 19,\n",
       " 17,\n",
       " 1,\n",
       " 19,\n",
       " 13,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 16,\n",
       " 10,\n",
       " 10,\n",
       " 17,\n",
       " 19,\n",
       " 5,\n",
       " 11,\n",
       " 19,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 13,\n",
       " 10,\n",
       " 15,\n",
       " 4,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 11,\n",
       " 10,\n",
       " 6,\n",
       " 16,\n",
       " 12,\n",
       " 6,\n",
       " 13,\n",
       " 18,\n",
       " 10,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 10,\n",
       " 5,\n",
       " 13,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 13,\n",
       " 17,\n",
       " 4,\n",
       " 14,\n",
       " 18,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 16,\n",
       " 9,\n",
       " 17,\n",
       " 10,\n",
       " 6,\n",
       " 19,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 10,\n",
       " 18,\n",
       " 18,\n",
       " 11,\n",
       " 14,\n",
       " 1,\n",
       " 14,\n",
       " 16,\n",
       " 2,\n",
       " 10,\n",
       " 18,\n",
       " 12,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 13,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 11,\n",
       " 16,\n",
       " 1,\n",
       " 3,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 5,\n",
       " 11,\n",
       " 17,\n",
       " 19,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 17,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 14,\n",
       " 16,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 6,\n",
       " 7,\n",
       " 13,\n",
       " 15,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 12,\n",
       " 1,\n",
       " 17,\n",
       " 19,\n",
       " 4,\n",
       " 5,\n",
       " 19,\n",
       " 13,\n",
       " 6,\n",
       " 6,\n",
       " 14,\n",
       " 0,\n",
       " 19,\n",
       " 13,\n",
       " 5,\n",
       " 13,\n",
       " 1,\n",
       " 17,\n",
       " 16,\n",
       " 4,\n",
       " 6,\n",
       " 16,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 10,\n",
       " 10,\n",
       " 19,\n",
       " 13,\n",
       " 1,\n",
       " 14,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 15,\n",
       " 15,\n",
       " 18,\n",
       " 8,\n",
       " 16,\n",
       " 14,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 14,\n",
       " 19,\n",
       " 6,\n",
       " 19,\n",
       " 2,\n",
       " 15,\n",
       " 15,\n",
       " 8,\n",
       " 10,\n",
       " 4,\n",
       " 18,\n",
       " 11,\n",
       " 16,\n",
       " 4,\n",
       " 11,\n",
       " 17,\n",
       " 6,\n",
       " 17,\n",
       " 16,\n",
       " 1,\n",
       " 14,\n",
       " 9,\n",
       " 18,\n",
       " 13,\n",
       " 9,\n",
       " 16,\n",
       " 12,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 17,\n",
       " 9,\n",
       " 6,\n",
       " 14,\n",
       " 11,\n",
       " 6,\n",
       " 6,\n",
       " 15,\n",
       " 12,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 16,\n",
       " 9,\n",
       " 15,\n",
       " 2,\n",
       " 19,\n",
       " 6,\n",
       " 14,\n",
       " 18,\n",
       " 12,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 16,\n",
       " 11,\n",
       " 15,\n",
       " 17,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 19,\n",
       " 2,\n",
       " 10,\n",
       " 2,\n",
       " 14,\n",
       " 19,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 14,\n",
       " 18,\n",
       " 5,\n",
       " 16,\n",
       " 3,\n",
       " 12,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 10,\n",
       " 4,\n",
       " 15,\n",
       " 13,\n",
       " 19,\n",
       " 4,\n",
       " 3,\n",
       " 11,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 14,\n",
       " 2,\n",
       " 15,\n",
       " 15,\n",
       " 13,\n",
       " 11,\n",
       " 4,\n",
       " 16,\n",
       " 9,\n",
       " 17,\n",
       " 12,\n",
       " 15,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 19,\n",
       " 16,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 2,\n",
       " 4,\n",
       " 12,\n",
       " 18,\n",
       " 4,\n",
       " 15,\n",
       " 12,\n",
       " 14,\n",
       " 4,\n",
       " 12,\n",
       " 19,\n",
       " 7,\n",
       " 12,\n",
       " 0,\n",
       " 16,\n",
       " 19,\n",
       " 6,\n",
       " 7,\n",
       " 18,\n",
       " 3,\n",
       " 8,\n",
       " 17,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 12,\n",
       " 10,\n",
       " 5,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 0,\n",
       " 13,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 9,\n",
       " 14,\n",
       " 12,\n",
       " 17,\n",
       " 19,\n",
       " 12,\n",
       " 12,\n",
       " 10,\n",
       " 16,\n",
       " 13,\n",
       " 2,\n",
       " 7,\n",
       " 18,\n",
       " 16,\n",
       " 16,\n",
       " 13,\n",
       " 3,\n",
       " 10,\n",
       " 9,\n",
       " 13,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 19,\n",
       " 19,\n",
       " 11,\n",
       " 5,\n",
       " 16,\n",
       " 14,\n",
       " 10,\n",
       " 16,\n",
       " 6,\n",
       " 1,\n",
       " 14,\n",
       " 4,\n",
       " 9,\n",
       " 13,\n",
       " 2,\n",
       " 16,\n",
       " 12,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 11,\n",
       " 2,\n",
       " 5,\n",
       " 19,\n",
       " 4,\n",
       " 7,\n",
       " 11,\n",
       " 6,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 16,\n",
       " 9,\n",
       " 16,\n",
       " 13,\n",
       " 14,\n",
       " 8,\n",
       " 19,\n",
       " 14,\n",
       " 3,\n",
       " 12,\n",
       " 0,\n",
       " 15,\n",
       " 13,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 14,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 15,\n",
       " 15,\n",
       " 2,\n",
       " 6,\n",
       " 19,\n",
       " 8,\n",
       " 12,\n",
       " 5,\n",
       " 2,\n",
       " 15,\n",
       " 4,\n",
       " 0,\n",
       " 11,\n",
       " 16,\n",
       " 10,\n",
       " 6,\n",
       " 18,\n",
       " 18,\n",
       " 10,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 11,\n",
       " 3,\n",
       " 14,\n",
       " 15,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 19,\n",
       " 12,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 14,\n",
       " 13,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 11,\n",
       " 7,\n",
       " 16,\n",
       " 12,\n",
       " 1,\n",
       " 7,\n",
       " 13,\n",
       " 18,\n",
       " 18,\n",
       " 1,\n",
       " 3,\n",
       " 15,\n",
       " 14,\n",
       " 8,\n",
       " 11,\n",
       " 4,\n",
       " 12,\n",
       " 17,\n",
       " 12,\n",
       " 19,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 15,\n",
       " 8,\n",
       " 17,\n",
       " 16,\n",
       " 18,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 10,\n",
       " 6,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 18,\n",
       " 10,\n",
       " 19,\n",
       " 17,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_y=trainset[b'coarse_labels']\n",
    "trainset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_class=len(meta[b'coarse_label_names'])\n",
    "n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#同理，把测试集也定义出来\n",
    "testset=unpickle(\"./cifar-100-python/test\")\n",
    "testset_x=testset[b'data']\n",
    "testset_y=testset[b'coarse_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便处理，将数据集图像数据转成RGB图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_x=trainset_x.reshape(-1,3,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_x=np.rollaxis(trainset_x, 1, 4)\n",
    "trainset_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_x=testset_x.reshape(-1,3,32,32)\n",
    "testset_x=np.rollaxis(testset_x, 1, 4)\n",
    "testset_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "b'small_mammals'\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "index = random.randint(0, len(trainset_x))\n",
    "plt.figure()\n",
    "plt.imshow(trainset_x[index])\n",
    "print(trainset_y[index])\n",
    "print(meta[b'coarse_label_names'][trainset_y[index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设计网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "current_num = 0\n",
    "\n",
    "def CLAHE(img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl1 = clahe.apply(img)\n",
    "    return cl1\n",
    "\n",
    "def Histograms_Equalization(img):\n",
    "    equ = cv2.equalizeHist(img)\n",
    "    return equ\n",
    "\n",
    "def make_one_hot(data, num):\n",
    "    return (np.arange(num)==data[:,None]).astype(np.integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7f6693f09a0e>:20: DeprecationWarning: Converting `np.integer` or `np.signedinteger` to a dtype is deprecated. The current result is `np.dtype(np.int_)` which is not strictly correct. Note that the result depends on the system. To ensure stable results use may want to use `np.int64` or `np.int32`.\n",
      "  return (np.arange(num)==data[:,None]).astype(np.integer)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "new_X_train = []\n",
    "new_y_train = []\n",
    "for index in range(len(trainset_x)):\n",
    "    sys.stdout.write(\" {} / {}\\r\".format(index, len(trainset_x)))\n",
    "\n",
    "    img_gray = cv2.cvtColor(trainset_x[index], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    new_X_train.append(img_gray.astype('float32') / 255.0)\n",
    "    new_y_train.append(trainset_y[index])\n",
    "    \n",
    "    he_image = Histograms_Equalization(img_gray)\n",
    "    new_X_train.append(he_image.astype('float32') / 255.0)\n",
    "    new_y_train.append(trainset_y[index])\n",
    "    \n",
    "    clahe_img = CLAHE(img_gray)\n",
    "    new_X_train.append(clahe_img.astype('float32') / 255.0)\n",
    "    new_y_train.append(trainset_y[index])\n",
    "    \n",
    "    \n",
    "print(\"All done!\")\n",
    "all_xs = np.expand_dims(new_X_train, 3)\n",
    "all_ys = make_one_hot(np.array(new_y_train), n_class)\n",
    "train_xs, valid_xs, train_ys, valid_ys = train_test_split(\n",
    "    all_xs, all_ys, test_size=0.2, random_state=0)\n",
    "# all_xs, all_ys = shuffle(np.array(new_X_train), np.array(new_y_train), random_state=0)\n",
    "\n",
    "pickle.dump(all_xs, open('all_xs.p', 'wb'))\n",
    "pickle.dump(all_ys, open('all_ys.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有个warning，但不影响程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x230445db0a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#index = 17245 \n",
    "#index = 12457 、\n",
    "index = 23456\n",
    "img_gray = cv2.cvtColor(trainset_x[index], cv2.COLOR_RGB2GRAY)\n",
    "plt.figure()\n",
    "plt.imshow(img_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23044727760>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#index = 17245 \n",
    "#index = 12457\n",
    "index = 23456\n",
    "img_gray = cv2.cvtColor(trainset_x[index], cv2.COLOR_RGB2GRAY)\n",
    "# img_gray = Histograms_Equalization(img_gray)\n",
    "img_gray = CLAHE(img_gray)\n",
    "plt.figure()\n",
    "plt.imshow(img_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7f6693f09a0e>:20: DeprecationWarning: Converting `np.integer` or `np.signedinteger` to a dtype is deprecated. The current result is `np.dtype(np.int_)` which is not strictly correct. Note that the result depends on the system. To ensure stable results use may want to use `np.int64` or `np.int32`.\n",
      "  return (np.arange(num)==data[:,None]).astype(np.integer)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "test_set = []\n",
    "for j in range(len(testset_x)):\n",
    "    img_gray = cv2.cvtColor(testset_x[j], cv2.COLOR_RGB2GRAY)\n",
    "#     img_gray = Histograms_Equalization(img_gray)\n",
    "    img_gray = np.expand_dims(img_gray, 2)\n",
    "    img_gray = img_gray / 255.0\n",
    "    test_set.append(img_gray)\n",
    "\n",
    "test_set = np.array(test_set)\n",
    "y_test = make_one_hot(np.array(testset_y), n_class)\n",
    "\n",
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里先是用tensorflow的1代，2代里面有几个函数被删除了，后面会根据TensorFlow2的特性再设计一个网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import datetime\n",
    "\n",
    "class network(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.x = tf.placeholder(tf.float32, shape=[None, 32, 32, 1], name='input_tensor')\n",
    "            self.y = tf.placeholder(tf.float32, shape=[None, n_class], name='labels')\n",
    "            \n",
    "            conv1 = tf.layers.conv2d(self.x, filters=100, kernel_size=5, activation=tf.nn.relu)            \n",
    "            print(\"conv1.shape = \", conv1.get_shape())  # (_, 28, 28, 100)    \n",
    "            self.conv1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2)\n",
    "\n",
    "            conv2 = tf.layers.conv2d(self.conv1, filters=150, kernel_size=3, activation=tf.nn.relu)            \n",
    "            print(\"conv2.shape = \", conv2.get_shape())  # (_, 12, 12, 150)     \n",
    "            self.conv2 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "            print(\"max_pool conv2.shape = \", self.conv2.get_shape())  # (?, 6, 6, 150)  \n",
    "\n",
    "            conv3 = tf.layers.conv2d(self.conv2, filters=250, kernel_size=3, padding='same', activation=tf.nn.relu)            \n",
    "            print(\"conv3.shape = \", conv3.get_shape())  # (_, 6, 6, 250)  \n",
    "            self.conv3 = tf.layers.max_pooling2d(conv3, pool_size=2, strides=2)\n",
    "            print(\"max_pool conv3.shape = \", self.conv3.get_shape())  # (_, 3, 3, 250)  \n",
    "\n",
    "            self.fc0   = tf.layers.flatten(self.conv3)\n",
    "            print(\"fc0.shape = \", self.fc0.get_shape())  # (_, 2250)  \n",
    "\n",
    "            self.fc1 = tf.layers.dense(self.fc0, units=512, activation=tf.nn.relu)\n",
    "\n",
    "            self.fc2 = tf.layers.dense(self.fc1, units=300, activation=tf.nn.relu)\n",
    "            \n",
    "            self.logits = tf.layers.dense(self.fc2, units=n_class)\n",
    "\n",
    "            with tf.name_scope('loss'):\n",
    "                self.cross_entropy = tf.reduce_mean(\n",
    "                    tf.losses.softmax_cross_entropy(self.y, logits=self.logits))\n",
    "\n",
    "            with tf.name_scope('train_step'):\n",
    "                self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.cross_entropy)\n",
    "\n",
    "            with tf.name_scope('accuracy'):\n",
    "                correct_prediction = tf.equal(tf.argmax(self.logits,1), tf.argmax(self.y,1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.sess = tf.Session(graph=self.graph)\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            self.save_dir = './session'\n",
    "            if not os.path.isdir(self.save_dir):\n",
    "                os.mkdir(self.save_dir)\n",
    "            self.ckpt = tf.train.get_checkpoint_state(self.save_dir)\n",
    "            if self.ckpt and self.ckpt.model_checkpoint_path:\n",
    "                self.saver.restore(self.sess, tf.train.latest_checkpoint(self.save_dir))\n",
    "                print(\"Successfully loaded:\", tf.train.latest_checkpoint(self.save_dir))\n",
    "            else:\n",
    "                print(\"Could not find old network weights\")\n",
    "\n",
    "    def training(self, xs, labels, ii, epoch_i, batch_i, batch_num):\n",
    "        feed_dict = {\n",
    "            self.x: xs,\n",
    "            self.y: labels\n",
    "        }\n",
    "\n",
    "        _, accuracy, loss = self.sess.run([self.train_step, self.accuracy, self.cross_entropy], feed_dict=feed_dict)\n",
    "        if (ii % 20 == 0):\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print('Training {}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.5f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    batch_num,\n",
    "                    loss))\n",
    "\n",
    "        return accuracy, loss\n",
    "    \n",
    "    def testing(self, xs, labels, ii, epoch_i, batch_i, batch_num):\n",
    "        feed_dict = {\n",
    "            self.x: xs,\n",
    "            self.y: labels\n",
    "        }\n",
    "\n",
    "        accuracy, loss = self.sess.run([self.accuracy, self.cross_entropy], feed_dict=feed_dict)\n",
    "        if (ii % 20 == 0):\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print('#Testing# {}: Epoch {:>3} Batch {:>4}/{} accuracy = {:.3f}  test_loss = {:.5f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    batch_num,\n",
    "                    accuracy,\n",
    "                    loss))\n",
    "\n",
    "        return accuracy, loss\n",
    "            \n",
    "    def save(self):\n",
    "        save_path = self.saver.save(self.sess, os.path.join(self.save_dir, 'best_model.ckpt'))    \n",
    "        print(\"Model saved in file: {}\".format(save_path))\n",
    "        \n",
    "    def forward(self, xs):  \n",
    "        feed_dict = {\n",
    "            self.x: xs\n",
    "        }\n",
    "        logits = self.sess.run([tf.nn.softmax(self.logits)], feed_dict=feed_dict)\n",
    "        logits = np.reshape(np.array(logits), (n_classes))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-4234775e7210>:13: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  conv1 = tf.layers.conv2d(self.x, filters=100, kernel_size=5, activation=tf.nn.relu)\n",
      "<ipython-input-44-4234775e7210>:15: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  self.conv1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2)\n",
      "<ipython-input-44-4234775e7210>:17: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  conv2 = tf.layers.conv2d(self.conv1, filters=150, kernel_size=3, activation=tf.nn.relu)\n",
      "<ipython-input-44-4234775e7210>:19: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  self.conv2 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
      "<ipython-input-44-4234775e7210>:22: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  conv3 = tf.layers.conv2d(self.conv2, filters=250, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
      "<ipython-input-44-4234775e7210>:24: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  self.conv3 = tf.layers.max_pooling2d(conv3, pool_size=2, strides=2)\n",
      "<ipython-input-44-4234775e7210>:27: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  self.fc0   = tf.layers.flatten(self.conv3)\n",
      "<ipython-input-44-4234775e7210>:30: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.fc1 = tf.layers.dense(self.fc0, units=512, activation=tf.nn.relu)\n",
      "<ipython-input-44-4234775e7210>:32: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.fc2 = tf.layers.dense(self.fc1, units=300, activation=tf.nn.relu)\n",
      "<ipython-input-44-4234775e7210>:34: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.logits = tf.layers.dense(self.fc2, units=n_class)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.shape =  (?, 28, 28, 100)\n",
      "conv2.shape =  (?, 12, 12, 150)\n",
      "max_pool conv2.shape =  (?, 6, 6, 150)\n",
      "conv3.shape =  (?, 6, 6, 250)\n",
      "max_pool conv3.shape =  (?, 3, 3, 250)\n",
      "fc0.shape =  (?, 2250)\n",
      "INFO:tensorflow:Restoring parameters from ./session\\best_model.ckpt\n",
      "Successfully loaded: ./session\\best_model.ckpt\n",
      "Training 2022-02-21T16:39:37.737976: Epoch   0 Batch    0/1875   train_loss = 2.18888\n",
      "Training 2022-02-21T16:39:40.277417: Epoch   0 Batch   20/1875   train_loss = 2.35947\n",
      "Training 2022-02-21T16:39:42.591483: Epoch   0 Batch   40/1875   train_loss = 1.95382\n",
      "Training 2022-02-21T16:39:45.200070: Epoch   0 Batch   60/1875   train_loss = 2.05660\n",
      "Training 2022-02-21T16:39:48.219413: Epoch   0 Batch   80/1875   train_loss = 2.07522\n",
      "Training 2022-02-21T16:39:50.849920: Epoch   0 Batch  100/1875   train_loss = 2.01661\n",
      "Training 2022-02-21T16:39:53.935499: Epoch   0 Batch  120/1875   train_loss = 1.98994\n",
      "Training 2022-02-21T16:39:56.411752: Epoch   0 Batch  140/1875   train_loss = 1.95196\n",
      "Training 2022-02-21T16:39:58.645306: Epoch   0 Batch  160/1875   train_loss = 1.85279\n",
      "Training 2022-02-21T16:40:01.089495: Epoch   0 Batch  180/1875   train_loss = 2.02479\n",
      "Training 2022-02-21T16:40:03.260404: Epoch   0 Batch  200/1875   train_loss = 1.97669\n",
      "Training 2022-02-21T16:40:05.577054: Epoch   0 Batch  220/1875   train_loss = 1.90044\n",
      "Training 2022-02-21T16:40:08.059503: Epoch   0 Batch  240/1875   train_loss = 1.93973\n",
      "Training 2022-02-21T16:40:10.493022: Epoch   0 Batch  260/1875   train_loss = 1.85274\n",
      "Training 2022-02-21T16:40:13.026839: Epoch   0 Batch  280/1875   train_loss = 1.85234\n",
      "Training 2022-02-21T16:40:15.441936: Epoch   0 Batch  300/1875   train_loss = 1.82729\n",
      "Training 2022-02-21T16:40:17.719728: Epoch   0 Batch  320/1875   train_loss = 1.90735\n",
      "Training 2022-02-21T16:40:20.006448: Epoch   0 Batch  340/1875   train_loss = 2.18042\n",
      "Training 2022-02-21T16:40:22.512897: Epoch   0 Batch  360/1875   train_loss = 2.11224\n",
      "Training 2022-02-21T16:40:26.206668: Epoch   0 Batch  380/1875   train_loss = 2.20850\n",
      "Training 2022-02-21T16:40:29.217081: Epoch   0 Batch  400/1875   train_loss = 1.90027\n",
      "Training 2022-02-21T16:40:31.898073: Epoch   0 Batch  420/1875   train_loss = 2.13556\n",
      "Training 2022-02-21T16:40:35.157863: Epoch   0 Batch  440/1875   train_loss = 1.73574\n",
      "Training 2022-02-21T16:40:37.686145: Epoch   0 Batch  460/1875   train_loss = 2.14228\n",
      "Training 2022-02-21T16:40:40.331647: Epoch   0 Batch  480/1875   train_loss = 2.15104\n",
      "Training 2022-02-21T16:40:43.171325: Epoch   0 Batch  500/1875   train_loss = 1.76736\n",
      "Training 2022-02-21T16:40:46.202370: Epoch   0 Batch  520/1875   train_loss = 1.87019\n",
      "Training 2022-02-21T16:40:49.172880: Epoch   0 Batch  540/1875   train_loss = 1.83646\n",
      "Training 2022-02-21T16:40:52.050951: Epoch   0 Batch  560/1875   train_loss = 1.75766\n",
      "Training 2022-02-21T16:40:55.433388: Epoch   0 Batch  580/1875   train_loss = 1.90604\n",
      "Training 2022-02-21T16:40:59.164339: Epoch   0 Batch  600/1875   train_loss = 1.84494\n",
      "Training 2022-02-21T16:41:02.465965: Epoch   0 Batch  620/1875   train_loss = 2.08953\n",
      "Training 2022-02-21T16:41:06.064512: Epoch   0 Batch  640/1875   train_loss = 1.86867\n",
      "Training 2022-02-21T16:41:09.612407: Epoch   0 Batch  660/1875   train_loss = 2.30392\n",
      "Training 2022-02-21T16:41:12.947486: Epoch   0 Batch  680/1875   train_loss = 1.93020\n",
      "Training 2022-02-21T16:41:16.157547: Epoch   0 Batch  700/1875   train_loss = 1.84135\n",
      "Training 2022-02-21T16:41:18.826864: Epoch   0 Batch  720/1875   train_loss = 1.83242\n",
      "Training 2022-02-21T16:41:21.445372: Epoch   0 Batch  740/1875   train_loss = 2.02323\n",
      "Training 2022-02-21T16:41:24.883815: Epoch   0 Batch  760/1875   train_loss = 1.80233\n",
      "Training 2022-02-21T16:41:28.502843: Epoch   0 Batch  780/1875   train_loss = 2.03334\n",
      "Training 2022-02-21T16:41:31.964590: Epoch   0 Batch  800/1875   train_loss = 2.01386\n",
      "Training 2022-02-21T16:41:34.718228: Epoch   0 Batch  820/1875   train_loss = 1.78407\n",
      "Training 2022-02-21T16:41:37.358648: Epoch   0 Batch  840/1875   train_loss = 1.82632\n",
      "Training 2022-02-21T16:41:40.090834: Epoch   0 Batch  860/1875   train_loss = 2.04690\n",
      "Training 2022-02-21T16:41:43.344062: Epoch   0 Batch  880/1875   train_loss = 1.91561\n",
      "Training 2022-02-21T16:41:46.921574: Epoch   0 Batch  900/1875   train_loss = 1.85180\n",
      "Training 2022-02-21T16:41:50.627670: Epoch   0 Batch  920/1875   train_loss = 2.02525\n",
      "Training 2022-02-21T16:41:53.439944: Epoch   0 Batch  940/1875   train_loss = 1.87242\n",
      "Training 2022-02-21T16:41:56.163684: Epoch   0 Batch  960/1875   train_loss = 1.71372\n",
      "Training 2022-02-21T16:41:58.890037: Epoch   0 Batch  980/1875   train_loss = 2.00313\n",
      "Training 2022-02-21T16:42:02.131446: Epoch   0 Batch 1000/1875   train_loss = 1.91302\n",
      "Training 2022-02-21T16:42:04.868254: Epoch   0 Batch 1020/1875   train_loss = 1.97896\n",
      "Training 2022-02-21T16:42:07.445572: Epoch   0 Batch 1040/1875   train_loss = 1.94494\n",
      "Training 2022-02-21T16:42:10.112598: Epoch   0 Batch 1060/1875   train_loss = 1.99431\n",
      "Training 2022-02-21T16:42:12.643259: Epoch   0 Batch 1080/1875   train_loss = 1.80373\n",
      "Training 2022-02-21T16:42:15.270923: Epoch   0 Batch 1100/1875   train_loss = 1.72468\n",
      "Training 2022-02-21T16:42:17.532924: Epoch   0 Batch 1120/1875   train_loss = 1.83199\n",
      "Training 2022-02-21T16:42:20.040008: Epoch   0 Batch 1140/1875   train_loss = 2.06596\n",
      "Training 2022-02-21T16:42:22.379371: Epoch   0 Batch 1160/1875   train_loss = 1.89044\n",
      "Training 2022-02-21T16:42:24.683643: Epoch   0 Batch 1180/1875   train_loss = 1.94071\n",
      "Training 2022-02-21T16:42:27.412925: Epoch   0 Batch 1200/1875   train_loss = 1.64933\n",
      "Training 2022-02-21T16:42:30.622380: Epoch   0 Batch 1220/1875   train_loss = 1.89734\n",
      "Training 2022-02-21T16:42:33.582346: Epoch   0 Batch 1240/1875   train_loss = 1.93091\n",
      "Training 2022-02-21T16:42:36.075597: Epoch   0 Batch 1260/1875   train_loss = 1.93531\n",
      "Training 2022-02-21T16:42:38.818626: Epoch   0 Batch 1280/1875   train_loss = 1.91576\n",
      "Training 2022-02-21T16:42:41.576251: Epoch   0 Batch 1300/1875   train_loss = 1.68946\n",
      "Training 2022-02-21T16:42:44.759742: Epoch   0 Batch 1320/1875   train_loss = 2.13114\n",
      "Training 2022-02-21T16:42:48.266338: Epoch   0 Batch 1340/1875   train_loss = 1.78076\n",
      "Training 2022-02-21T16:42:52.079306: Epoch   0 Batch 1360/1875   train_loss = 1.80658\n",
      "Training 2022-02-21T16:42:55.006481: Epoch   0 Batch 1380/1875   train_loss = 1.90723\n",
      "Training 2022-02-21T16:42:57.666372: Epoch   0 Batch 1400/1875   train_loss = 2.28435\n",
      "Training 2022-02-21T16:43:00.107845: Epoch   0 Batch 1420/1875   train_loss = 1.84906\n",
      "Training 2022-02-21T16:43:03.050978: Epoch   0 Batch 1440/1875   train_loss = 1.78239\n",
      "Training 2022-02-21T16:43:05.443583: Epoch   0 Batch 1460/1875   train_loss = 1.87168\n",
      "Training 2022-02-21T16:43:07.952834: Epoch   0 Batch 1480/1875   train_loss = 1.84161\n",
      "Training 2022-02-21T16:43:10.525997: Epoch   0 Batch 1500/1875   train_loss = 1.89560\n",
      "Training 2022-02-21T16:43:12.988415: Epoch   0 Batch 1520/1875   train_loss = 1.68898\n",
      "Training 2022-02-21T16:43:16.065191: Epoch   0 Batch 1540/1875   train_loss = 2.04276\n",
      "Training 2022-02-21T16:43:19.033258: Epoch   0 Batch 1560/1875   train_loss = 1.80951\n",
      "Training 2022-02-21T16:43:21.844744: Epoch   0 Batch 1580/1875   train_loss = 2.03179\n",
      "Training 2022-02-21T16:43:24.703104: Epoch   0 Batch 1600/1875   train_loss = 1.84871\n",
      "Training 2022-02-21T16:43:27.625295: Epoch   0 Batch 1620/1875   train_loss = 2.13687\n",
      "Training 2022-02-21T16:43:30.195419: Epoch   0 Batch 1640/1875   train_loss = 1.49732\n",
      "Training 2022-02-21T16:43:32.430453: Epoch   0 Batch 1660/1875   train_loss = 1.65716\n",
      "Training 2022-02-21T16:43:34.645525: Epoch   0 Batch 1680/1875   train_loss = 1.86012\n",
      "Training 2022-02-21T16:43:36.968315: Epoch   0 Batch 1700/1875   train_loss = 1.68073\n",
      "Training 2022-02-21T16:43:39.313995: Epoch   0 Batch 1720/1875   train_loss = 1.99214\n",
      "Training 2022-02-21T16:43:41.632169: Epoch   0 Batch 1740/1875   train_loss = 1.76864\n",
      "Training 2022-02-21T16:43:44.004815: Epoch   0 Batch 1760/1875   train_loss = 1.79712\n",
      "Training 2022-02-21T16:43:46.320641: Epoch   0 Batch 1780/1875   train_loss = 2.08952\n",
      "Training 2022-02-21T16:43:49.797392: Epoch   0 Batch 1800/1875   train_loss = 1.90401\n",
      "Training 2022-02-21T16:43:53.054779: Epoch   0 Batch 1820/1875   train_loss = 1.76461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2022-02-21T16:43:56.202370: Epoch   0 Batch 1840/1875   train_loss = 1.87058\n",
      "Training 2022-02-21T16:43:58.721632: Epoch   0 Batch 1860/1875   train_loss = 2.01757\n",
      "#Testing# 2022-02-21T16:44:00.525810: Epoch   0 Batch    0/468 accuracy = 0.484  test_loss = 1.67643\n",
      "#Testing# 2022-02-21T16:44:01.038439: Epoch   0 Batch   20/468 accuracy = 0.375  test_loss = 1.86075\n",
      "#Testing# 2022-02-21T16:44:01.531122: Epoch   0 Batch   40/468 accuracy = 0.344  test_loss = 1.99714\n",
      "#Testing# 2022-02-21T16:44:02.023807: Epoch   0 Batch   60/468 accuracy = 0.359  test_loss = 2.18973\n",
      "#Testing# 2022-02-21T16:44:02.493551: Epoch   0 Batch   80/468 accuracy = 0.469  test_loss = 1.74681\n",
      "#Testing# 2022-02-21T16:44:02.996206: Epoch   0 Batch  100/468 accuracy = 0.453  test_loss = 1.93121\n",
      "#Testing# 2022-02-21T16:44:03.595604: Epoch   0 Batch  120/468 accuracy = 0.312  test_loss = 2.02127\n",
      "#Testing# 2022-02-21T16:44:04.052386: Epoch   0 Batch  140/468 accuracy = 0.344  test_loss = 1.99814\n",
      "#Testing# 2022-02-21T16:44:04.498191: Epoch   0 Batch  160/468 accuracy = 0.422  test_loss = 2.06191\n",
      "#Testing# 2022-02-21T16:44:04.954970: Epoch   0 Batch  180/468 accuracy = 0.453  test_loss = 1.84315\n",
      "#Testing# 2022-02-21T16:44:05.396789: Epoch   0 Batch  200/468 accuracy = 0.391  test_loss = 1.84050\n",
      "#Testing# 2022-02-21T16:44:05.854565: Epoch   0 Batch  220/468 accuracy = 0.328  test_loss = 2.08463\n",
      "#Testing# 2022-02-21T16:44:06.337276: Epoch   0 Batch  240/468 accuracy = 0.453  test_loss = 2.00986\n",
      "#Testing# 2022-02-21T16:44:06.815996: Epoch   0 Batch  260/468 accuracy = 0.453  test_loss = 1.79894\n",
      "#Testing# 2022-02-21T16:44:07.309677: Epoch   0 Batch  280/468 accuracy = 0.453  test_loss = 1.86579\n",
      "#Testing# 2022-02-21T16:44:07.796601: Epoch   0 Batch  300/468 accuracy = 0.438  test_loss = 1.68103\n",
      "#Testing# 2022-02-21T16:44:08.282305: Epoch   0 Batch  320/468 accuracy = 0.531  test_loss = 1.73774\n",
      "#Testing# 2022-02-21T16:44:08.770007: Epoch   0 Batch  340/468 accuracy = 0.375  test_loss = 2.02566\n",
      "#Testing# 2022-02-21T16:44:09.248243: Epoch   0 Batch  360/468 accuracy = 0.406  test_loss = 1.86964\n",
      "#Testing# 2022-02-21T16:44:09.749288: Epoch   0 Batch  380/468 accuracy = 0.438  test_loss = 1.90004\n",
      "#Testing# 2022-02-21T16:44:10.285209: Epoch   0 Batch  400/468 accuracy = 0.438  test_loss = 1.85118\n",
      "#Testing# 2022-02-21T16:44:10.872489: Epoch   0 Batch  420/468 accuracy = 0.531  test_loss = 1.68697\n",
      "#Testing# 2022-02-21T16:44:11.359189: Epoch   0 Batch  440/468 accuracy = 0.500  test_loss = 1.71981\n",
      "#Testing# 2022-02-21T16:44:11.858853: Epoch   0 Batch  460/468 accuracy = 0.328  test_loss = 2.19684\n",
      "best loss = 1.9635909469718607  acc = 0.3976028311965812\n",
      "Model saved in file: ./session\\best_model.ckpt\n",
      "Training 2022-02-21T16:44:12.801156: Epoch   1 Batch    5/1875   train_loss = 1.91332\n",
      "Training 2022-02-21T16:44:14.857659: Epoch   1 Batch   25/1875   train_loss = 1.81691\n",
      "Training 2022-02-21T16:44:16.916172: Epoch   1 Batch   45/1875   train_loss = 1.97126\n",
      "Training 2022-02-21T16:44:19.136450: Epoch   1 Batch   65/1875   train_loss = 2.20326\n",
      "Training 2022-02-21T16:44:21.560743: Epoch   1 Batch   85/1875   train_loss = 1.81938\n",
      "Training 2022-02-21T16:44:24.392175: Epoch   1 Batch  105/1875   train_loss = 1.90586\n",
      "Training 2022-02-21T16:44:26.789765: Epoch   1 Batch  125/1875   train_loss = 2.01675\n",
      "Training 2022-02-21T16:44:29.346933: Epoch   1 Batch  145/1875   train_loss = 1.89194\n",
      "Training 2022-02-21T16:44:31.886142: Epoch   1 Batch  165/1875   train_loss = 1.88091\n",
      "Training 2022-02-21T16:44:34.350556: Epoch   1 Batch  185/1875   train_loss = 1.69064\n",
      "Training 2022-02-21T16:44:36.561644: Epoch   1 Batch  205/1875   train_loss = 1.70071\n",
      "Training 2022-02-21T16:44:38.768141: Epoch   1 Batch  225/1875   train_loss = 1.59087\n",
      "Training 2022-02-21T16:44:41.220657: Epoch   1 Batch  245/1875   train_loss = 1.81551\n",
      "Training 2022-02-21T16:44:44.097035: Epoch   1 Batch  265/1875   train_loss = 2.02342\n",
      "Training 2022-02-21T16:44:46.495024: Epoch   1 Batch  285/1875   train_loss = 1.92155\n",
      "Training 2022-02-21T16:44:48.950796: Epoch   1 Batch  305/1875   train_loss = 1.73994\n",
      "Training 2022-02-21T16:44:51.153877: Epoch   1 Batch  325/1875   train_loss = 1.85161\n",
      "Training 2022-02-21T16:44:53.258654: Epoch   1 Batch  345/1875   train_loss = 2.22125\n",
      "Training 2022-02-21T16:44:55.428271: Epoch   1 Batch  365/1875   train_loss = 1.92680\n",
      "Training 2022-02-21T16:44:57.958410: Epoch   1 Batch  385/1875   train_loss = 1.81653\n",
      "Training 2022-02-21T16:45:00.673410: Epoch   1 Batch  405/1875   train_loss = 1.86379\n",
      "Training 2022-02-21T16:45:03.025012: Epoch   1 Batch  425/1875   train_loss = 1.87469\n",
      "Training 2022-02-21T16:45:05.187553: Epoch   1 Batch  445/1875   train_loss = 1.87684\n",
      "Training 2022-02-21T16:45:07.216151: Epoch   1 Batch  465/1875   train_loss = 1.43686\n",
      "Training 2022-02-21T16:45:09.188202: Epoch   1 Batch  485/1875   train_loss = 1.86655\n",
      "Training 2022-02-21T16:45:11.484924: Epoch   1 Batch  505/1875   train_loss = 1.62873\n",
      "Training 2022-02-21T16:45:14.526790: Epoch   1 Batch  525/1875   train_loss = 1.83589\n",
      "Training 2022-02-21T16:45:17.287761: Epoch   1 Batch  545/1875   train_loss = 2.01720\n",
      "Training 2022-02-21T16:45:19.767921: Epoch   1 Batch  565/1875   train_loss = 1.70571\n",
      "Training 2022-02-21T16:45:22.073329: Epoch   1 Batch  585/1875   train_loss = 1.88630\n",
      "Training 2022-02-21T16:45:24.309353: Epoch   1 Batch  605/1875   train_loss = 2.14493\n",
      "Training 2022-02-21T16:45:26.349899: Epoch   1 Batch  625/1875   train_loss = 1.87551\n",
      "Training 2022-02-21T16:45:28.592903: Epoch   1 Batch  645/1875   train_loss = 1.64532\n",
      "Training 2022-02-21T16:45:31.338563: Epoch   1 Batch  665/1875   train_loss = 1.79176\n",
      "Training 2022-02-21T16:45:34.356499: Epoch   1 Batch  685/1875   train_loss = 1.85432\n",
      "Training 2022-02-21T16:45:36.831837: Epoch   1 Batch  705/1875   train_loss = 1.97045\n",
      "Training 2022-02-21T16:45:38.926285: Epoch   1 Batch  725/1875   train_loss = 1.92938\n",
      "Training 2022-02-21T16:45:40.994932: Epoch   1 Batch  745/1875   train_loss = 1.44892\n",
      "Training 2022-02-21T16:45:43.188588: Epoch   1 Batch  765/1875   train_loss = 2.02456\n",
      "Training 2022-02-21T16:45:45.529581: Epoch   1 Batch  785/1875   train_loss = 1.94124\n",
      "Training 2022-02-21T16:45:47.996989: Epoch   1 Batch  805/1875   train_loss = 1.85543\n",
      "Training 2022-02-21T16:45:50.307130: Epoch   1 Batch  825/1875   train_loss = 1.73254\n",
      "Training 2022-02-21T16:45:52.581502: Epoch   1 Batch  845/1875   train_loss = 1.96316\n",
      "Training 2022-02-21T16:45:54.758683: Epoch   1 Batch  865/1875   train_loss = 1.71142\n",
      "Training 2022-02-21T16:45:56.851090: Epoch   1 Batch  885/1875   train_loss = 1.62882\n",
      "Training 2022-02-21T16:45:59.000347: Epoch   1 Batch  905/1875   train_loss = 1.89810\n",
      "Training 2022-02-21T16:46:01.471740: Epoch   1 Batch  925/1875   train_loss = 1.71007\n",
      "Training 2022-02-21T16:46:03.780570: Epoch   1 Batch  945/1875   train_loss = 1.77966\n",
      "Training 2022-02-21T16:46:05.847047: Epoch   1 Batch  965/1875   train_loss = 1.95318\n",
      "Training 2022-02-21T16:46:07.911526: Epoch   1 Batch  985/1875   train_loss = 1.82524\n",
      "Training 2022-02-21T16:46:09.984984: Epoch   1 Batch 1005/1875   train_loss = 1.67257\n",
      "Training 2022-02-21T16:46:12.244944: Epoch   1 Batch 1025/1875   train_loss = 1.59804\n",
      "Training 2022-02-21T16:46:14.302445: Epoch   1 Batch 1045/1875   train_loss = 1.96015\n",
      "Training 2022-02-21T16:46:16.684079: Epoch   1 Batch 1065/1875   train_loss = 1.74154\n",
      "Training 2022-02-21T16:46:19.530075: Epoch   1 Batch 1085/1875   train_loss = 1.76209\n",
      "Training 2022-02-21T16:46:21.731688: Epoch   1 Batch 1105/1875   train_loss = 1.70045\n",
      "Training 2022-02-21T16:46:23.808615: Epoch   1 Batch 1125/1875   train_loss = 1.93448\n",
      "Training 2022-02-21T16:46:25.939839: Epoch   1 Batch 1145/1875   train_loss = 1.90073\n",
      "Training 2022-02-21T16:46:27.992354: Epoch   1 Batch 1165/1875   train_loss = 1.58882\n",
      "Training 2022-02-21T16:46:30.374987: Epoch   1 Batch 1185/1875   train_loss = 1.85099\n",
      "Training 2022-02-21T16:46:33.126630: Epoch   1 Batch 1205/1875   train_loss = 1.85504\n",
      "Training 2022-02-21T16:46:35.786519: Epoch   1 Batch 1225/1875   train_loss = 1.91431\n",
      "Training 2022-02-21T16:46:38.551735: Epoch   1 Batch 1245/1875   train_loss = 1.79315\n",
      "Training 2022-02-21T16:46:41.238551: Epoch   1 Batch 1265/1875   train_loss = 1.77558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2022-02-21T16:46:43.939332: Epoch   1 Batch 1285/1875   train_loss = 2.15779\n",
      "Training 2022-02-21T16:46:46.133467: Epoch   1 Batch 1305/1875   train_loss = 1.71196\n",
      "Training 2022-02-21T16:46:48.407576: Epoch   1 Batch 1325/1875   train_loss = 1.68145\n",
      "Training 2022-02-21T16:46:50.745002: Epoch   1 Batch 1345/1875   train_loss = 1.63610\n",
      "Training 2022-02-21T16:46:53.141637: Epoch   1 Batch 1365/1875   train_loss = 2.04071\n",
      "Training 2022-02-21T16:46:55.828472: Epoch   1 Batch 1385/1875   train_loss = 1.66923\n",
      "Training 2022-02-21T16:46:58.020621: Epoch   1 Batch 1405/1875   train_loss = 1.92650\n",
      "Training 2022-02-21T16:47:01.091412: Epoch   1 Batch 1425/1875   train_loss = 1.67436\n",
      "Training 2022-02-21T16:47:03.855027: Epoch   1 Batch 1445/1875   train_loss = 1.91139\n",
      "Training 2022-02-21T16:47:06.311464: Epoch   1 Batch 1465/1875   train_loss = 1.89961\n",
      "Training 2022-02-21T16:47:09.400115: Epoch   1 Batch 1485/1875   train_loss = 1.55014\n",
      "Training 2022-02-21T16:47:12.383192: Epoch   1 Batch 1505/1875   train_loss = 2.11198\n",
      "Training 2022-02-21T16:47:15.158222: Epoch   1 Batch 1525/1875   train_loss = 1.62837\n",
      "Training 2022-02-21T16:47:18.075429: Epoch   1 Batch 1545/1875   train_loss = 1.87127\n",
      "Training 2022-02-21T16:47:20.585717: Epoch   1 Batch 1565/1875   train_loss = 1.78772\n",
      "Training 2022-02-21T16:47:22.995116: Epoch   1 Batch 1585/1875   train_loss = 1.97897\n",
      "Training 2022-02-21T16:47:26.031997: Epoch   1 Batch 1605/1875   train_loss = 1.86497\n",
      "Training 2022-02-21T16:47:28.971140: Epoch   1 Batch 1625/1875   train_loss = 1.85561\n",
      "Training 2022-02-21T16:47:31.836483: Epoch   1 Batch 1645/1875   train_loss = 1.60456\n",
      "Training 2022-02-21T16:47:34.405613: Epoch   1 Batch 1665/1875   train_loss = 1.74247\n",
      "Training 2022-02-21T16:47:36.622689: Epoch   1 Batch 1685/1875   train_loss = 1.61271\n",
      "Training 2022-02-21T16:47:38.783914: Epoch   1 Batch 1705/1875   train_loss = 1.56915\n",
      "Training 2022-02-21T16:47:40.901250: Epoch   1 Batch 1725/1875   train_loss = 1.50160\n",
      "Training 2022-02-21T16:47:43.040613: Epoch   1 Batch 1745/1875   train_loss = 1.56539\n",
      "Training 2022-02-21T16:47:45.109555: Epoch   1 Batch 1765/1875   train_loss = 1.67185\n",
      "Training 2022-02-21T16:47:47.146495: Epoch   1 Batch 1785/1875   train_loss = 2.08916\n",
      "Training 2022-02-21T16:47:49.259203: Epoch   1 Batch 1805/1875   train_loss = 1.68974\n",
      "Training 2022-02-21T16:47:51.418361: Epoch   1 Batch 1825/1875   train_loss = 1.79708\n",
      "Training 2022-02-21T16:47:53.760241: Epoch   1 Batch 1845/1875   train_loss = 1.87678\n",
      "Training 2022-02-21T16:47:55.936313: Epoch   1 Batch 1865/1875   train_loss = 1.87738\n",
      "#Testing# 2022-02-21T16:47:57.180091: Epoch   1 Batch   12/468 accuracy = 0.500  test_loss = 1.60389\n",
      "#Testing# 2022-02-21T16:47:57.668785: Epoch   1 Batch   32/468 accuracy = 0.484  test_loss = 1.73918\n",
      "#Testing# 2022-02-21T16:47:58.148503: Epoch   1 Batch   52/468 accuracy = 0.422  test_loss = 1.98659\n",
      "#Testing# 2022-02-21T16:47:58.639194: Epoch   1 Batch   72/468 accuracy = 0.391  test_loss = 2.17685\n",
      "#Testing# 2022-02-21T16:47:59.115918: Epoch   1 Batch   92/468 accuracy = 0.469  test_loss = 1.69953\n",
      "#Testing# 2022-02-21T16:47:59.593640: Epoch   1 Batch  112/468 accuracy = 0.422  test_loss = 1.77905\n",
      "#Testing# 2022-02-21T16:48:00.092308: Epoch   1 Batch  132/468 accuracy = 0.453  test_loss = 1.83184\n",
      "#Testing# 2022-02-21T16:48:00.613916: Epoch   1 Batch  152/468 accuracy = 0.484  test_loss = 1.62292\n",
      "#Testing# 2022-02-21T16:48:01.136515: Epoch   1 Batch  172/468 accuracy = 0.500  test_loss = 1.71403\n",
      "#Testing# 2022-02-21T16:48:01.608256: Epoch   1 Batch  192/468 accuracy = 0.469  test_loss = 1.79207\n",
      "#Testing# 2022-02-21T16:48:02.077998: Epoch   1 Batch  212/468 accuracy = 0.469  test_loss = 1.84273\n",
      "#Testing# 2022-02-21T16:48:02.539764: Epoch   1 Batch  232/468 accuracy = 0.531  test_loss = 1.75209\n",
      "#Testing# 2022-02-21T16:48:03.017488: Epoch   1 Batch  252/468 accuracy = 0.406  test_loss = 1.71965\n",
      "#Testing# 2022-02-21T16:48:03.475264: Epoch   1 Batch  272/468 accuracy = 0.484  test_loss = 1.74759\n",
      "#Testing# 2022-02-21T16:48:03.934037: Epoch   1 Batch  292/468 accuracy = 0.500  test_loss = 1.71624\n",
      "#Testing# 2022-02-21T16:48:04.391816: Epoch   1 Batch  312/468 accuracy = 0.328  test_loss = 2.15356\n",
      "#Testing# 2022-02-21T16:48:04.848593: Epoch   1 Batch  332/468 accuracy = 0.406  test_loss = 1.85348\n",
      "#Testing# 2022-02-21T16:48:05.305372: Epoch   1 Batch  352/468 accuracy = 0.469  test_loss = 1.70811\n",
      "#Testing# 2022-02-21T16:48:05.758164: Epoch   1 Batch  372/468 accuracy = 0.375  test_loss = 1.90759\n",
      "#Testing# 2022-02-21T16:48:06.261039: Epoch   1 Batch  392/468 accuracy = 0.500  test_loss = 1.76906\n",
      "#Testing# 2022-02-21T16:48:06.786635: Epoch   1 Batch  412/468 accuracy = 0.438  test_loss = 1.74630\n",
      "#Testing# 2022-02-21T16:48:07.303253: Epoch   1 Batch  432/468 accuracy = 0.453  test_loss = 1.97025\n",
      "#Testing# 2022-02-21T16:48:07.790950: Epoch   1 Batch  452/468 accuracy = 0.391  test_loss = 2.00049\n",
      "best loss = 1.8806872421350234  acc = 0.4243456196581197\n",
      "Model saved in file: ./session\\best_model.ckpt\n",
      "Training 2022-02-21T16:48:09.433056: Epoch   2 Batch   10/1875   train_loss = 1.53724\n",
      "Training 2022-02-21T16:48:11.686036: Epoch   2 Batch   30/1875   train_loss = 1.88298\n",
      "Training 2022-02-21T16:48:14.028772: Epoch   2 Batch   50/1875   train_loss = 1.67668\n",
      "Training 2022-02-21T16:48:16.112912: Epoch   2 Batch   70/1875   train_loss = 1.78815\n",
      "Training 2022-02-21T16:48:18.216302: Epoch   2 Batch   90/1875   train_loss = 1.60148\n",
      "Training 2022-02-21T16:48:20.408443: Epoch   2 Batch  110/1875   train_loss = 1.73726\n",
      "Training 2022-02-21T16:48:22.677375: Epoch   2 Batch  130/1875   train_loss = 1.66497\n",
      "Training 2022-02-21T16:48:24.901429: Epoch   2 Batch  150/1875   train_loss = 1.60558\n",
      "Training 2022-02-21T16:48:27.322545: Epoch   2 Batch  170/1875   train_loss = 1.95922\n",
      "Training 2022-02-21T16:48:29.438889: Epoch   2 Batch  190/1875   train_loss = 1.95264\n",
      "Training 2022-02-21T16:48:31.526309: Epoch   2 Batch  210/1875   train_loss = 1.74137\n",
      "Training 2022-02-21T16:48:33.630683: Epoch   2 Batch  230/1875   train_loss = 1.44849\n",
      "Training 2022-02-21T16:48:35.677212: Epoch   2 Batch  250/1875   train_loss = 1.90283\n",
      "Training 2022-02-21T16:48:37.770089: Epoch   2 Batch  270/1875   train_loss = 1.50706\n",
      "Training 2022-02-21T16:48:39.931798: Epoch   2 Batch  290/1875   train_loss = 1.46070\n",
      "Training 2022-02-21T16:48:42.072446: Epoch   2 Batch  310/1875   train_loss = 2.06332\n",
      "Training 2022-02-21T16:48:44.357751: Epoch   2 Batch  330/1875   train_loss = 1.49471\n",
      "Training 2022-02-21T16:48:47.248372: Epoch   2 Batch  350/1875   train_loss = 1.64737\n",
      "Training 2022-02-21T16:48:50.122050: Epoch   2 Batch  370/1875   train_loss = 1.55389\n",
      "Training 2022-02-21T16:48:53.405281: Epoch   2 Batch  390/1875   train_loss = 1.60234\n",
      "Training 2022-02-21T16:48:57.483374: Epoch   2 Batch  410/1875   train_loss = 1.79065\n",
      "Training 2022-02-21T16:49:00.540203: Epoch   2 Batch  430/1875   train_loss = 1.76175\n",
      "Training 2022-02-21T16:49:03.910205: Epoch   2 Batch  450/1875   train_loss = 1.47096\n",
      "Training 2022-02-21T16:49:06.724675: Epoch   2 Batch  470/1875   train_loss = 1.84793\n",
      "Training 2022-02-21T16:49:09.733631: Epoch   2 Batch  490/1875   train_loss = 1.63827\n",
      "Training 2022-02-21T16:49:12.535144: Epoch   2 Batch  510/1875   train_loss = 1.44624\n",
      "Training 2022-02-21T16:49:16.039529: Epoch   2 Batch  530/1875   train_loss = 1.84174\n",
      "Training 2022-02-21T16:49:18.834899: Epoch   2 Batch  550/1875   train_loss = 1.49413\n",
      "Training 2022-02-21T16:49:21.768933: Epoch   2 Batch  570/1875   train_loss = 1.73165\n",
      "Training 2022-02-21T16:49:24.381422: Epoch   2 Batch  590/1875   train_loss = 1.74705\n",
      "Training 2022-02-21T16:49:26.569268: Epoch   2 Batch  610/1875   train_loss = 1.87840\n",
      "Training 2022-02-21T16:49:28.872676: Epoch   2 Batch  630/1875   train_loss = 1.53253\n",
      "Training 2022-02-21T16:49:31.639163: Epoch   2 Batch  650/1875   train_loss = 1.53455\n",
      "Training 2022-02-21T16:49:34.244050: Epoch   2 Batch  670/1875   train_loss = 1.65150\n",
      "Training 2022-02-21T16:49:37.237385: Epoch   2 Batch  690/1875   train_loss = 1.49582\n",
      "Training 2022-02-21T16:49:39.787583: Epoch   2 Batch  710/1875   train_loss = 1.62354\n",
      "Training 2022-02-21T16:49:41.966633: Epoch   2 Batch  730/1875   train_loss = 1.67569\n",
      "Training 2022-02-21T16:49:44.580853: Epoch   2 Batch  750/1875   train_loss = 1.49894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2022-02-21T16:49:46.993692: Epoch   2 Batch  770/1875   train_loss = 1.56472\n",
      "Training 2022-02-21T16:49:49.291607: Epoch   2 Batch  790/1875   train_loss = 1.71612\n",
      "Training 2022-02-21T16:49:51.966843: Epoch   2 Batch  810/1875   train_loss = 1.62686\n",
      "Training 2022-02-21T16:49:54.630957: Epoch   2 Batch  830/1875   train_loss = 1.98031\n",
      "Training 2022-02-21T16:49:57.079413: Epoch   2 Batch  850/1875   train_loss = 1.64834\n",
      "Training 2022-02-21T16:49:59.831058: Epoch   2 Batch  870/1875   train_loss = 1.63927\n",
      "Training 2022-02-21T16:50:03.027526: Epoch   2 Batch  890/1875   train_loss = 1.63158\n",
      "Training 2022-02-21T16:50:05.834020: Epoch   2 Batch  910/1875   train_loss = 1.59206\n",
      "Training 2022-02-21T16:50:08.636527: Epoch   2 Batch  930/1875   train_loss = 1.60953\n",
      "Training 2022-02-21T16:50:11.214637: Epoch   2 Batch  950/1875   train_loss = 1.72858\n",
      "Training 2022-02-21T16:50:13.560366: Epoch   2 Batch  970/1875   train_loss = 1.67984\n",
      "Training 2022-02-21T16:50:16.480560: Epoch   2 Batch  990/1875   train_loss = 1.49113\n",
      "Training 2022-02-21T16:50:19.012793: Epoch   2 Batch 1010/1875   train_loss = 1.52095\n",
      "Training 2022-02-21T16:50:21.466233: Epoch   2 Batch 1030/1875   train_loss = 1.88071\n",
      "Training 2022-02-21T16:50:23.772070: Epoch   2 Batch 1050/1875   train_loss = 1.75873\n",
      "Training 2022-02-21T16:50:26.084888: Epoch   2 Batch 1070/1875   train_loss = 1.58667\n",
      "Training 2022-02-21T16:50:28.577226: Epoch   2 Batch 1090/1875   train_loss = 1.83230\n",
      "Training 2022-02-21T16:50:31.240110: Epoch   2 Batch 1110/1875   train_loss = 1.75671\n",
      "Training 2022-02-21T16:50:34.467485: Epoch   2 Batch 1130/1875   train_loss = 1.51409\n",
      "Training 2022-02-21T16:50:37.607096: Epoch   2 Batch 1150/1875   train_loss = 1.64560\n",
      "Training 2022-02-21T16:50:40.265985: Epoch   2 Batch 1170/1875   train_loss = 1.58095\n",
      "Training 2022-02-21T16:50:42.621685: Epoch   2 Batch 1190/1875   train_loss = 1.32656\n",
      "Training 2022-02-21T16:50:44.950808: Epoch   2 Batch 1210/1875   train_loss = 1.82315\n",
      "Training 2022-02-21T16:50:47.574081: Epoch   2 Batch 1230/1875   train_loss = 1.67749\n",
      "Training 2022-02-21T16:50:49.996022: Epoch   2 Batch 1250/1875   train_loss = 1.46898\n",
      "Training 2022-02-21T16:50:52.914262: Epoch   2 Batch 1270/1875   train_loss = 1.51676\n",
      "Training 2022-02-21T16:50:55.447506: Epoch   2 Batch 1290/1875   train_loss = 1.53892\n",
      "Training 2022-02-21T16:50:57.962307: Epoch   2 Batch 1310/1875   train_loss = 1.73174\n",
      "Training 2022-02-21T16:51:00.306779: Epoch   2 Batch 1330/1875   train_loss = 1.54245\n",
      "Training 2022-02-21T16:51:02.589520: Epoch   2 Batch 1350/1875   train_loss = 1.81597\n",
      "Training 2022-02-21T16:51:05.284786: Epoch   2 Batch 1370/1875   train_loss = 1.40255\n",
      "Training 2022-02-21T16:51:08.012325: Epoch   2 Batch 1390/1875   train_loss = 1.83707\n",
      "Training 2022-02-21T16:51:10.824810: Epoch   2 Batch 1410/1875   train_loss = 1.87425\n",
      "Training 2022-02-21T16:51:13.688831: Epoch   2 Batch 1430/1875   train_loss = 1.65583\n",
      "Training 2022-02-21T16:51:17.012790: Epoch   2 Batch 1450/1875   train_loss = 1.96479\n",
      "Training 2022-02-21T16:51:20.912211: Epoch   2 Batch 1470/1875   train_loss = 1.47792\n",
      "Training 2022-02-21T16:51:23.894478: Epoch   2 Batch 1490/1875   train_loss = 1.61235\n",
      "Training 2022-02-21T16:51:27.473692: Epoch   2 Batch 1510/1875   train_loss = 1.73141\n",
      "Training 2022-02-21T16:51:30.252473: Epoch   2 Batch 1530/1875   train_loss = 1.70513\n",
      "Training 2022-02-21T16:51:32.836308: Epoch   2 Batch 1550/1875   train_loss = 1.75178\n",
      "Training 2022-02-21T16:51:35.715457: Epoch   2 Batch 1570/1875   train_loss = 1.56075\n",
      "Training 2022-02-21T16:51:38.950442: Epoch   2 Batch 1590/1875   train_loss = 1.89675\n",
      "Training 2022-02-21T16:51:42.516086: Epoch   2 Batch 1610/1875   train_loss = 1.68647\n",
      "Training 2022-02-21T16:51:46.840939: Epoch   2 Batch 1630/1875   train_loss = 1.57309\n",
      "Training 2022-02-21T16:51:50.888401: Epoch   2 Batch 1650/1875   train_loss = 1.62985\n",
      "Training 2022-02-21T16:51:54.313692: Epoch   2 Batch 1670/1875   train_loss = 1.40011\n",
      "Training 2022-02-21T16:51:57.205893: Epoch   2 Batch 1690/1875   train_loss = 1.68327\n",
      "Training 2022-02-21T16:51:59.721841: Epoch   2 Batch 1710/1875   train_loss = 1.90465\n",
      "Training 2022-02-21T16:52:02.407153: Epoch   2 Batch 1730/1875   train_loss = 1.48558\n",
      "Training 2022-02-21T16:52:04.887935: Epoch   2 Batch 1750/1875   train_loss = 1.72772\n",
      "Training 2022-02-21T16:52:07.796009: Epoch   2 Batch 1770/1875   train_loss = 1.75744\n",
      "Training 2022-02-21T16:52:10.980319: Epoch   2 Batch 1790/1875   train_loss = 1.76981\n",
      "Training 2022-02-21T16:52:14.059490: Epoch   2 Batch 1810/1875   train_loss = 1.60051\n",
      "Training 2022-02-21T16:52:16.879952: Epoch   2 Batch 1830/1875   train_loss = 1.79138\n",
      "Training 2022-02-21T16:52:19.044602: Epoch   2 Batch 1850/1875   train_loss = 1.36965\n",
      "Training 2022-02-21T16:52:21.137247: Epoch   2 Batch 1870/1875   train_loss = 1.43851\n",
      "#Testing# 2022-02-21T16:52:21.709047: Epoch   2 Batch    4/468 accuracy = 0.391  test_loss = 2.05636\n",
      "#Testing# 2022-02-21T16:52:22.210213: Epoch   2 Batch   24/468 accuracy = 0.422  test_loss = 1.81230\n",
      "#Testing# 2022-02-21T16:52:22.712405: Epoch   2 Batch   44/468 accuracy = 0.547  test_loss = 1.75750\n",
      "#Testing# 2022-02-21T16:52:23.226391: Epoch   2 Batch   64/468 accuracy = 0.438  test_loss = 1.90422\n",
      "#Testing# 2022-02-21T16:52:23.728722: Epoch   2 Batch   84/468 accuracy = 0.453  test_loss = 1.93130\n",
      "#Testing# 2022-02-21T16:52:24.224437: Epoch   2 Batch  104/468 accuracy = 0.516  test_loss = 1.69161\n",
      "#Testing# 2022-02-21T16:52:24.727502: Epoch   2 Batch  124/468 accuracy = 0.516  test_loss = 1.49585\n",
      "#Testing# 2022-02-21T16:52:25.204242: Epoch   2 Batch  144/468 accuracy = 0.438  test_loss = 1.90036\n",
      "#Testing# 2022-02-21T16:52:25.733879: Epoch   2 Batch  164/468 accuracy = 0.438  test_loss = 1.97320\n",
      "#Testing# 2022-02-21T16:52:26.262292: Epoch   2 Batch  184/468 accuracy = 0.359  test_loss = 2.09747\n",
      "#Testing# 2022-02-21T16:52:26.771872: Epoch   2 Batch  204/468 accuracy = 0.328  test_loss = 1.97894\n",
      "#Testing# 2022-02-21T16:52:27.259613: Epoch   2 Batch  224/468 accuracy = 0.516  test_loss = 1.57887\n",
      "#Testing# 2022-02-21T16:52:27.758278: Epoch   2 Batch  244/468 accuracy = 0.500  test_loss = 1.65116\n",
      "#Testing# 2022-02-21T16:52:28.265934: Epoch   2 Batch  264/468 accuracy = 0.484  test_loss = 1.74611\n",
      "#Testing# 2022-02-21T16:52:28.757071: Epoch   2 Batch  284/468 accuracy = 0.500  test_loss = 1.70644\n",
      "#Testing# 2022-02-21T16:52:29.246763: Epoch   2 Batch  304/468 accuracy = 0.469  test_loss = 1.64622\n",
      "#Testing# 2022-02-21T16:52:29.735456: Epoch   2 Batch  324/468 accuracy = 0.359  test_loss = 2.15732\n",
      "#Testing# 2022-02-21T16:52:30.227144: Epoch   2 Batch  344/468 accuracy = 0.484  test_loss = 1.80453\n",
      "#Testing# 2022-02-21T16:52:30.706860: Epoch   2 Batch  364/468 accuracy = 0.375  test_loss = 1.81726\n",
      "#Testing# 2022-02-21T16:52:31.190886: Epoch   2 Batch  384/468 accuracy = 0.469  test_loss = 1.58365\n",
      "#Testing# 2022-02-21T16:52:31.692545: Epoch   2 Batch  404/468 accuracy = 0.453  test_loss = 1.72348\n",
      "#Testing# 2022-02-21T16:52:32.152316: Epoch   2 Batch  424/468 accuracy = 0.469  test_loss = 1.65665\n",
      "#Testing# 2022-02-21T16:52:32.620065: Epoch   2 Batch  444/468 accuracy = 0.500  test_loss = 1.74544\n",
      "#Testing# 2022-02-21T16:52:33.091323: Epoch   2 Batch  464/468 accuracy = 0.438  test_loss = 1.96097\n",
      "best loss = 1.8105775153535044  acc = 0.4470486111111111\n",
      "Model saved in file: ./session\\best_model.ckpt\n",
      "Training 2022-02-21T16:52:34.893649: Epoch   3 Batch   15/1875   train_loss = 1.62442\n",
      "Training 2022-02-21T16:52:36.892241: Epoch   3 Batch   35/1875   train_loss = 1.61723\n",
      "Training 2022-02-21T16:52:38.957939: Epoch   3 Batch   55/1875   train_loss = 1.53227\n",
      "Training 2022-02-21T16:52:41.101199: Epoch   3 Batch   75/1875   train_loss = 1.49248\n",
      "Training 2022-02-21T16:52:43.501781: Epoch   3 Batch   95/1875   train_loss = 1.31730\n",
      "Training 2022-02-21T16:52:45.876328: Epoch   3 Batch  115/1875   train_loss = 1.48559\n",
      "Training 2022-02-21T16:52:48.166637: Epoch   3 Batch  135/1875   train_loss = 1.81928\n",
      "Training 2022-02-21T16:52:50.466358: Epoch   3 Batch  155/1875   train_loss = 1.40104\n",
      "Training 2022-02-21T16:52:52.924201: Epoch   3 Batch  175/1875   train_loss = 1.36652\n",
      "Training 2022-02-21T16:52:55.166046: Epoch   3 Batch  195/1875   train_loss = 1.69307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2022-02-21T16:52:57.295575: Epoch   3 Batch  215/1875   train_loss = 1.71307\n",
      "Training 2022-02-21T16:52:59.464218: Epoch   3 Batch  235/1875   train_loss = 1.71808\n",
      "Training 2022-02-21T16:53:01.647928: Epoch   3 Batch  255/1875   train_loss = 1.55396\n",
      "Training 2022-02-21T16:53:03.779729: Epoch   3 Batch  275/1875   train_loss = 1.88363\n",
      "Training 2022-02-21T16:53:05.820737: Epoch   3 Batch  295/1875   train_loss = 1.77346\n",
      "Training 2022-02-21T16:53:07.901065: Epoch   3 Batch  315/1875   train_loss = 2.04089\n",
      "Training 2022-02-21T16:53:10.505733: Epoch   3 Batch  335/1875   train_loss = 1.40364\n",
      "Training 2022-02-21T16:53:13.172918: Epoch   3 Batch  355/1875   train_loss = 1.44911\n",
      "Training 2022-02-21T16:53:15.852953: Epoch   3 Batch  375/1875   train_loss = 1.59013\n",
      "Training 2022-02-21T16:53:19.206040: Epoch   3 Batch  395/1875   train_loss = 1.72672\n",
      "Training 2022-02-21T16:53:21.703504: Epoch   3 Batch  415/1875   train_loss = 1.72485\n",
      "Training 2022-02-21T16:53:24.053226: Epoch   3 Batch  435/1875   train_loss = 1.55128\n",
      "Training 2022-02-21T16:53:26.422910: Epoch   3 Batch  455/1875   train_loss = 1.59645\n",
      "Training 2022-02-21T16:53:28.976096: Epoch   3 Batch  475/1875   train_loss = 1.54404\n",
      "Training 2022-02-21T16:53:31.268469: Epoch   3 Batch  495/1875   train_loss = 1.79090\n",
      "Training 2022-02-21T16:53:34.179684: Epoch   3 Batch  515/1875   train_loss = 1.49853\n",
      "Training 2022-02-21T16:53:36.707328: Epoch   3 Batch  535/1875   train_loss = 1.63165\n",
      "Training 2022-02-21T16:53:39.391484: Epoch   3 Batch  555/1875   train_loss = 1.69750\n",
      "Training 2022-02-21T16:53:41.752994: Epoch   3 Batch  575/1875   train_loss = 1.32890\n",
      "Training 2022-02-21T16:53:44.583422: Epoch   3 Batch  595/1875   train_loss = 1.63383\n",
      "Training 2022-02-21T16:53:46.975030: Epoch   3 Batch  615/1875   train_loss = 1.49444\n",
      "Training 2022-02-21T16:53:49.492307: Epoch   3 Batch  635/1875   train_loss = 1.58824\n",
      "Training 2022-02-21T16:53:52.074582: Epoch   3 Batch  655/1875   train_loss = 1.35426\n",
      "Training 2022-02-21T16:53:54.456327: Epoch   3 Batch  675/1875   train_loss = 1.54692\n",
      "Training 2022-02-21T16:53:57.029450: Epoch   3 Batch  695/1875   train_loss = 1.52691\n",
      "Training 2022-02-21T16:53:59.445990: Epoch   3 Batch  715/1875   train_loss = 1.51413\n",
      "Training 2022-02-21T16:54:01.961267: Epoch   3 Batch  735/1875   train_loss = 1.88358\n",
      "Training 2022-02-21T16:54:04.227210: Epoch   3 Batch  755/1875   train_loss = 1.29473\n",
      "Training 2022-02-21T16:54:06.333580: Epoch   3 Batch  775/1875   train_loss = 1.41527\n",
      "Training 2022-02-21T16:54:08.401055: Epoch   3 Batch  795/1875   train_loss = 1.75569\n",
      "Training 2022-02-21T16:54:10.528367: Epoch   3 Batch  815/1875   train_loss = 1.59289\n",
      "Training 2022-02-21T16:54:12.861138: Epoch   3 Batch  835/1875   train_loss = 1.72496\n",
      "Training 2022-02-21T16:54:15.194968: Epoch   3 Batch  855/1875   train_loss = 1.50977\n",
      "Training 2022-02-21T16:54:17.456922: Epoch   3 Batch  875/1875   train_loss = 1.62256\n",
      "Training 2022-02-21T16:54:19.752784: Epoch   3 Batch  895/1875   train_loss = 1.78306\n",
      "Training 2022-02-21T16:54:21.908023: Epoch   3 Batch  915/1875   train_loss = 1.52980\n",
      "Training 2022-02-21T16:54:24.071241: Epoch   3 Batch  935/1875   train_loss = 1.87186\n",
      "Training 2022-02-21T16:54:26.193084: Epoch   3 Batch  955/1875   train_loss = 1.30558\n",
      "Training 2022-02-21T16:54:28.270541: Epoch   3 Batch  975/1875   train_loss = 1.37083\n",
      "Training 2022-02-21T16:54:30.405133: Epoch   3 Batch  995/1875   train_loss = 1.49925\n",
      "Training 2022-02-21T16:54:32.656093: Epoch   3 Batch 1015/1875   train_loss = 1.51359\n",
      "Training 2022-02-21T16:54:34.966394: Epoch   3 Batch 1035/1875   train_loss = 1.19463\n",
      "Training 2022-02-21T16:54:37.217377: Epoch   3 Batch 1055/1875   train_loss = 1.97725\n",
      "Training 2022-02-21T16:54:40.054321: Epoch   3 Batch 1075/1875   train_loss = 1.80718\n",
      "Training 2022-02-21T16:54:42.801723: Epoch   3 Batch 1095/1875   train_loss = 1.71728\n",
      "Training 2022-02-21T16:54:45.807305: Epoch   3 Batch 1115/1875   train_loss = 1.34152\n",
      "Training 2022-02-21T16:54:48.808654: Epoch   3 Batch 1135/1875   train_loss = 1.60429\n",
      "Training 2022-02-21T16:54:52.066945: Epoch   3 Batch 1155/1875   train_loss = 1.64896\n",
      "Training 2022-02-21T16:54:54.900370: Epoch   3 Batch 1175/1875   train_loss = 1.75311\n",
      "Training 2022-02-21T16:54:57.306939: Epoch   3 Batch 1195/1875   train_loss = 1.43713\n",
      "Training 2022-02-21T16:54:59.663638: Epoch   3 Batch 1215/1875   train_loss = 1.78906\n",
      "Training 2022-02-21T16:55:02.155980: Epoch   3 Batch 1235/1875   train_loss = 1.46272\n",
      "Training 2022-02-21T16:55:04.579498: Epoch   3 Batch 1255/1875   train_loss = 1.90182\n",
      "Training 2022-02-21T16:55:07.109736: Epoch   3 Batch 1275/1875   train_loss = 1.60808\n",
      "Training 2022-02-21T16:55:09.622020: Epoch   3 Batch 1295/1875   train_loss = 1.45078\n",
      "Training 2022-02-21T16:55:12.006647: Epoch   3 Batch 1315/1875   train_loss = 1.38292\n",
      "Training 2022-02-21T16:55:14.436154: Epoch   3 Batch 1335/1875   train_loss = 1.51286\n",
      "Training 2022-02-21T16:55:16.789093: Epoch   3 Batch 1355/1875   train_loss = 1.38510\n",
      "Training 2022-02-21T16:55:19.120188: Epoch   3 Batch 1375/1875   train_loss = 1.37468\n",
      "Training 2022-02-21T16:55:21.619006: Epoch   3 Batch 1395/1875   train_loss = 1.68655\n",
      "Training 2022-02-21T16:55:23.991667: Epoch   3 Batch 1415/1875   train_loss = 1.26990\n",
      "Training 2022-02-21T16:55:26.263948: Epoch   3 Batch 1435/1875   train_loss = 1.53224\n",
      "Training 2022-02-21T16:55:28.519592: Epoch   3 Batch 1455/1875   train_loss = 1.59612\n",
      "Training 2022-02-21T16:55:30.742715: Epoch   3 Batch 1475/1875   train_loss = 1.56018\n",
      "Training 2022-02-21T16:55:33.006519: Epoch   3 Batch 1495/1875   train_loss = 1.52886\n",
      "Training 2022-02-21T16:55:35.316772: Epoch   3 Batch 1515/1875   train_loss = 1.42305\n",
      "Training 2022-02-21T16:55:37.661503: Epoch   3 Batch 1535/1875   train_loss = 1.69603\n",
      "Training 2022-02-21T16:55:40.034699: Epoch   3 Batch 1555/1875   train_loss = 1.84110\n",
      "Training 2022-02-21T16:55:42.303766: Epoch   3 Batch 1575/1875   train_loss = 1.57591\n",
      "Training 2022-02-21T16:55:44.597504: Epoch   3 Batch 1595/1875   train_loss = 1.40465\n",
      "Training 2022-02-21T16:55:46.825463: Epoch   3 Batch 1615/1875   train_loss = 1.42039\n",
      "Training 2022-02-21T16:55:49.075798: Epoch   3 Batch 1635/1875   train_loss = 1.34282\n",
      "Training 2022-02-21T16:55:51.417553: Epoch   3 Batch 1655/1875   train_loss = 1.44501\n",
      "Training 2022-02-21T16:55:53.857561: Epoch   3 Batch 1675/1875   train_loss = 1.33763\n",
      "Training 2022-02-21T16:55:56.302012: Epoch   3 Batch 1695/1875   train_loss = 1.39268\n",
      "Training 2022-02-21T16:55:58.761402: Epoch   3 Batch 1715/1875   train_loss = 1.91714\n",
      "Training 2022-02-21T16:56:01.167408: Epoch   3 Batch 1735/1875   train_loss = 1.44332\n",
      "Training 2022-02-21T16:56:03.545092: Epoch   3 Batch 1755/1875   train_loss = 1.27619\n",
      "Training 2022-02-21T16:56:05.887979: Epoch   3 Batch 1775/1875   train_loss = 1.57657\n",
      "Training 2022-02-21T16:56:08.319728: Epoch   3 Batch 1795/1875   train_loss = 1.80332\n",
      "Training 2022-02-21T16:56:10.796608: Epoch   3 Batch 1815/1875   train_loss = 1.45215\n",
      "Training 2022-02-21T16:56:13.275263: Epoch   3 Batch 1835/1875   train_loss = 1.81733\n",
      "Training 2022-02-21T16:56:15.599839: Epoch   3 Batch 1855/1875   train_loss = 1.45082\n",
      "#Testing# 2022-02-21T16:56:18.268267: Epoch   3 Batch   16/468 accuracy = 0.531  test_loss = 1.64666\n",
      "#Testing# 2022-02-21T16:56:18.747997: Epoch   3 Batch   36/468 accuracy = 0.562  test_loss = 1.60165\n",
      "#Testing# 2022-02-21T16:56:19.227714: Epoch   3 Batch   56/468 accuracy = 0.500  test_loss = 1.73412\n",
      "#Testing# 2022-02-21T16:56:19.714412: Epoch   3 Batch   76/468 accuracy = 0.531  test_loss = 1.59215\n",
      "#Testing# 2022-02-21T16:56:20.194132: Epoch   3 Batch   96/468 accuracy = 0.453  test_loss = 1.72217\n",
      "#Testing# 2022-02-21T16:56:20.678836: Epoch   3 Batch  116/468 accuracy = 0.594  test_loss = 1.52204\n",
      "#Testing# 2022-02-21T16:56:21.159550: Epoch   3 Batch  136/468 accuracy = 0.438  test_loss = 1.77605\n",
      "#Testing# 2022-02-21T16:56:21.669695: Epoch   3 Batch  156/468 accuracy = 0.438  test_loss = 1.69766\n",
      "#Testing# 2022-02-21T16:56:22.205273: Epoch   3 Batch  176/468 accuracy = 0.406  test_loss = 2.19140\n",
      "#Testing# 2022-02-21T16:56:22.736852: Epoch   3 Batch  196/468 accuracy = 0.406  test_loss = 1.84724\n",
      "#Testing# 2022-02-21T16:56:23.258456: Epoch   3 Batch  216/468 accuracy = 0.375  test_loss = 1.79666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Testing# 2022-02-21T16:56:23.780061: Epoch   3 Batch  236/468 accuracy = 0.484  test_loss = 1.75687\n",
      "#Testing# 2022-02-21T16:56:24.303665: Epoch   3 Batch  256/468 accuracy = 0.484  test_loss = 1.75439\n",
      "#Testing# 2022-02-21T16:56:24.827270: Epoch   3 Batch  276/468 accuracy = 0.484  test_loss = 1.81230\n",
      "#Testing# 2022-02-21T16:56:25.321941: Epoch   3 Batch  296/468 accuracy = 0.375  test_loss = 1.83816\n",
      "#Testing# 2022-02-21T16:56:25.867482: Epoch   3 Batch  316/468 accuracy = 0.484  test_loss = 1.69411\n",
      "#Testing# 2022-02-21T16:56:26.417014: Epoch   3 Batch  336/468 accuracy = 0.500  test_loss = 1.67010\n",
      "#Testing# 2022-02-21T16:56:26.943606: Epoch   3 Batch  356/468 accuracy = 0.453  test_loss = 1.68927\n",
      "#Testing# 2022-02-21T16:56:27.447261: Epoch   3 Batch  376/468 accuracy = 0.438  test_loss = 1.75854\n",
      "#Testing# 2022-02-21T16:56:27.953907: Epoch   3 Batch  396/468 accuracy = 0.484  test_loss = 1.77565\n",
      "#Testing# 2022-02-21T16:56:28.457559: Epoch   3 Batch  416/468 accuracy = 0.422  test_loss = 1.93466\n",
      "#Testing# 2022-02-21T16:56:28.961213: Epoch   3 Batch  436/468 accuracy = 0.359  test_loss = 1.85002\n",
      "#Testing# 2022-02-21T16:56:29.466384: Epoch   3 Batch  456/468 accuracy = 0.359  test_loss = 2.20100\n",
      "best loss = 1.7828980905887408  acc = 0.45866720085470086\n",
      "Model saved in file: ./session\\best_model.ckpt\n",
      "Training 2022-02-21T16:56:30.007288: Epoch   4 Batch    0/1875   train_loss = 1.68364\n",
      "Training 2022-02-21T16:56:32.338059: Epoch   4 Batch   20/1875   train_loss = 1.88812\n",
      "Training 2022-02-21T16:56:34.618840: Epoch   4 Batch   40/1875   train_loss = 1.49415\n",
      "Training 2022-02-21T16:56:36.899247: Epoch   4 Batch   60/1875   train_loss = 1.61452\n",
      "Training 2022-02-21T16:56:39.278889: Epoch   4 Batch   80/1875   train_loss = 1.57770\n",
      "Training 2022-02-21T16:56:41.752339: Epoch   4 Batch  100/1875   train_loss = 1.67817\n",
      "Training 2022-02-21T16:56:44.234078: Epoch   4 Batch  120/1875   train_loss = 1.55737\n",
      "Training 2022-02-21T16:56:46.478260: Epoch   4 Batch  140/1875   train_loss = 1.43863\n",
      "Training 2022-02-21T16:56:48.708196: Epoch   4 Batch  160/1875   train_loss = 1.33136\n",
      "Training 2022-02-21T16:56:50.917370: Epoch   4 Batch  180/1875   train_loss = 1.35740\n",
      "Training 2022-02-21T16:56:53.211353: Epoch   4 Batch  200/1875   train_loss = 1.46559\n",
      "Training 2022-02-21T16:56:55.630890: Epoch   4 Batch  220/1875   train_loss = 1.49023\n",
      "Training 2022-02-21T16:56:58.075595: Epoch   4 Batch  240/1875   train_loss = 1.37072\n",
      "Training 2022-02-21T16:57:00.570374: Epoch   4 Batch  260/1875   train_loss = 1.46408\n",
      "Training 2022-02-21T16:57:03.683027: Epoch   4 Batch  280/1875   train_loss = 1.39519\n",
      "Training 2022-02-21T16:57:06.618257: Epoch   4 Batch  300/1875   train_loss = 1.42302\n",
      "Training 2022-02-21T16:57:09.220987: Epoch   4 Batch  320/1875   train_loss = 1.38245\n",
      "Training 2022-02-21T16:57:11.886981: Epoch   4 Batch  340/1875   train_loss = 1.63962\n",
      "Training 2022-02-21T16:57:14.559207: Epoch   4 Batch  360/1875   train_loss = 1.64281\n",
      "Training 2022-02-21T16:57:17.090957: Epoch   4 Batch  380/1875   train_loss = 1.76897\n",
      "Training 2022-02-21T16:57:19.200257: Epoch   4 Batch  400/1875   train_loss = 1.35984\n",
      "Training 2022-02-21T16:57:21.309068: Epoch   4 Batch  420/1875   train_loss = 1.73144\n",
      "Training 2022-02-21T16:57:23.397422: Epoch   4 Batch  440/1875   train_loss = 1.23785\n",
      "Training 2022-02-21T16:57:25.655389: Epoch   4 Batch  460/1875   train_loss = 1.73773\n",
      "Training 2022-02-21T16:57:27.946264: Epoch   4 Batch  480/1875   train_loss = 1.73296\n",
      "Training 2022-02-21T16:57:30.231870: Epoch   4 Batch  500/1875   train_loss = 1.29228\n",
      "Training 2022-02-21T16:57:32.595558: Epoch   4 Batch  520/1875   train_loss = 1.42229\n",
      "Training 2022-02-21T16:57:34.803123: Epoch   4 Batch  540/1875   train_loss = 1.33473\n",
      "Training 2022-02-21T16:57:36.934366: Epoch   4 Batch  560/1875   train_loss = 1.25790\n",
      "Training 2022-02-21T16:57:38.950512: Epoch   4 Batch  580/1875   train_loss = 1.50428\n",
      "Training 2022-02-21T16:57:40.983468: Epoch   4 Batch  600/1875   train_loss = 1.35442\n",
      "Training 2022-02-21T16:57:43.146483: Epoch   4 Batch  620/1875   train_loss = 1.74627\n",
      "Training 2022-02-21T16:57:45.333741: Epoch   4 Batch  640/1875   train_loss = 1.35536\n",
      "Training 2022-02-21T16:57:47.965078: Epoch   4 Batch  660/1875   train_loss = 1.98511\n",
      "Training 2022-02-21T16:57:50.341733: Epoch   4 Batch  680/1875   train_loss = 1.42881\n",
      "Training 2022-02-21T16:57:52.860493: Epoch   4 Batch  700/1875   train_loss = 1.22059\n",
      "Training 2022-02-21T16:57:54.987325: Epoch   4 Batch  720/1875   train_loss = 1.33728\n",
      "Training 2022-02-21T16:57:57.091421: Epoch   4 Batch  740/1875   train_loss = 1.39267\n",
      "Training 2022-02-21T16:57:59.353894: Epoch   4 Batch  760/1875   train_loss = 1.35984\n",
      "Training 2022-02-21T16:58:01.737495: Epoch   4 Batch  780/1875   train_loss = 1.49471\n",
      "Training 2022-02-21T16:58:04.037753: Epoch   4 Batch  800/1875   train_loss = 1.60292\n",
      "Training 2022-02-21T16:58:06.213841: Epoch   4 Batch  820/1875   train_loss = 1.31091\n",
      "Training 2022-02-21T16:58:08.402001: Epoch   4 Batch  840/1875   train_loss = 1.36700\n",
      "Training 2022-02-21T16:58:10.480529: Epoch   4 Batch  860/1875   train_loss = 1.61960\n",
      "Training 2022-02-21T16:58:12.508583: Epoch   4 Batch  880/1875   train_loss = 1.51597\n",
      "Training 2022-02-21T16:58:14.598261: Epoch   4 Batch  900/1875   train_loss = 1.25158\n",
      "Training 2022-02-21T16:58:16.797268: Epoch   4 Batch  920/1875   train_loss = 1.72239\n",
      "Training 2022-02-21T16:58:18.988350: Epoch   4 Batch  940/1875   train_loss = 1.33646\n",
      "Training 2022-02-21T16:58:21.104727: Epoch   4 Batch  960/1875   train_loss = 1.20923\n",
      "Training 2022-02-21T16:58:23.212112: Epoch   4 Batch  980/1875   train_loss = 1.60307\n",
      "Training 2022-02-21T16:58:25.253654: Epoch   4 Batch 1000/1875   train_loss = 1.47264\n",
      "Training 2022-02-21T16:58:27.734073: Epoch   4 Batch 1020/1875   train_loss = 1.58319\n",
      "Training 2022-02-21T16:58:30.623718: Epoch   4 Batch 1040/1875   train_loss = 1.50648\n",
      "Training 2022-02-21T16:58:34.111350: Epoch   4 Batch 1060/1875   train_loss = 1.54522\n",
      "Training 2022-02-21T16:58:37.096638: Epoch   4 Batch 1080/1875   train_loss = 1.44487\n",
      "Training 2022-02-21T16:58:39.852112: Epoch   4 Batch 1100/1875   train_loss = 1.13510\n",
      "Training 2022-02-21T16:58:42.904755: Epoch   4 Batch 1120/1875   train_loss = 1.31947\n",
      "Training 2022-02-21T16:58:45.490223: Epoch   4 Batch 1140/1875   train_loss = 1.42050\n",
      "Training 2022-02-21T16:58:48.294737: Epoch   4 Batch 1160/1875   train_loss = 1.40563\n",
      "Training 2022-02-21T16:58:50.645962: Epoch   4 Batch 1180/1875   train_loss = 1.46338\n",
      "Training 2022-02-21T16:58:53.138069: Epoch   4 Batch 1200/1875   train_loss = 1.31725\n",
      "Training 2022-02-21T16:58:55.510669: Epoch   4 Batch 1220/1875   train_loss = 1.34169\n",
      "Training 2022-02-21T16:58:57.824485: Epoch   4 Batch 1240/1875   train_loss = 1.57724\n",
      "Training 2022-02-21T16:59:00.327124: Epoch   4 Batch 1260/1875   train_loss = 1.47165\n",
      "Training 2022-02-21T16:59:02.753542: Epoch   4 Batch 1280/1875   train_loss = 1.59503\n",
      "Training 2022-02-21T16:59:05.137766: Epoch   4 Batch 1300/1875   train_loss = 1.16027\n",
      "Training 2022-02-21T16:59:07.457822: Epoch   4 Batch 1320/1875   train_loss = 1.46726\n",
      "Training 2022-02-21T16:59:09.594117: Epoch   4 Batch 1340/1875   train_loss = 1.33806\n",
      "Training 2022-02-21T16:59:11.719238: Epoch   4 Batch 1360/1875   train_loss = 1.24112\n",
      "Training 2022-02-21T16:59:13.795864: Epoch   4 Batch 1380/1875   train_loss = 1.46229\n",
      "Training 2022-02-21T16:59:15.849381: Epoch   4 Batch 1400/1875   train_loss = 1.77883\n",
      "Training 2022-02-21T16:59:17.901730: Epoch   4 Batch 1420/1875   train_loss = 1.34281\n",
      "Training 2022-02-21T16:59:20.049988: Epoch   4 Batch 1440/1875   train_loss = 1.45428\n",
      "Training 2022-02-21T16:59:22.244612: Epoch   4 Batch 1460/1875   train_loss = 1.48946\n",
      "Training 2022-02-21T16:59:24.436724: Epoch   4 Batch 1480/1875   train_loss = 1.42773\n",
      "Training 2022-02-21T16:59:26.512466: Epoch   4 Batch 1500/1875   train_loss = 1.40460\n",
      "Training 2022-02-21T16:59:28.577008: Epoch   4 Batch 1520/1875   train_loss = 1.26183\n",
      "Training 2022-02-21T16:59:30.592997: Epoch   4 Batch 1540/1875   train_loss = 1.55868\n",
      "Training 2022-02-21T16:59:32.655562: Epoch   4 Batch 1560/1875   train_loss = 1.39818\n",
      "Training 2022-02-21T16:59:34.772826: Epoch   4 Batch 1580/1875   train_loss = 1.49857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2022-02-21T16:59:36.944023: Epoch   4 Batch 1600/1875   train_loss = 1.64843\n",
      "Training 2022-02-21T16:59:39.204593: Epoch   4 Batch 1620/1875   train_loss = 1.74308\n",
      "Training 2022-02-21T16:59:41.297647: Epoch   4 Batch 1640/1875   train_loss = 0.99920\n",
      "Training 2022-02-21T16:59:43.407696: Epoch   4 Batch 1660/1875   train_loss = 1.21198\n",
      "Training 2022-02-21T16:59:45.452662: Epoch   4 Batch 1680/1875   train_loss = 1.34291\n",
      "Training 2022-02-21T16:59:47.495236: Epoch   4 Batch 1700/1875   train_loss = 1.12393\n",
      "Training 2022-02-21T16:59:49.525898: Epoch   4 Batch 1720/1875   train_loss = 1.56592\n",
      "Training 2022-02-21T16:59:51.917469: Epoch   4 Batch 1740/1875   train_loss = 1.39323\n",
      "Training 2022-02-21T16:59:54.324812: Epoch   4 Batch 1760/1875   train_loss = 1.33199\n",
      "Training 2022-02-21T16:59:57.086436: Epoch   4 Batch 1780/1875   train_loss = 1.62660\n",
      "Training 2022-02-21T16:59:59.250776: Epoch   4 Batch 1800/1875   train_loss = 1.46429\n",
      "Training 2022-02-21T17:00:01.622177: Epoch   4 Batch 1820/1875   train_loss = 1.24050\n",
      "Training 2022-02-21T17:00:04.631896: Epoch   4 Batch 1840/1875   train_loss = 1.33879\n",
      "Training 2022-02-21T17:00:06.980017: Epoch   4 Batch 1860/1875   train_loss = 1.53059\n",
      "#Testing# 2022-02-21T17:00:09.347260: Epoch   4 Batch    8/468 accuracy = 0.500  test_loss = 1.99904\n",
      "#Testing# 2022-02-21T17:00:09.961618: Epoch   4 Batch   28/468 accuracy = 0.484  test_loss = 2.08993\n",
      "#Testing# 2022-02-21T17:00:10.624190: Epoch   4 Batch   48/468 accuracy = 0.453  test_loss = 1.88259\n",
      "#Testing# 2022-02-21T17:00:11.350762: Epoch   4 Batch   68/468 accuracy = 0.328  test_loss = 1.87625\n",
      "#Testing# 2022-02-21T17:00:11.982075: Epoch   4 Batch   88/468 accuracy = 0.500  test_loss = 1.51303\n",
      "#Testing# 2022-02-21T17:00:12.558124: Epoch   4 Batch  108/468 accuracy = 0.469  test_loss = 1.91250\n",
      "#Testing# 2022-02-21T17:00:13.321086: Epoch   4 Batch  128/468 accuracy = 0.438  test_loss = 1.88712\n",
      "#Testing# 2022-02-21T17:00:13.854175: Epoch   4 Batch  148/468 accuracy = 0.531  test_loss = 1.70410\n",
      "#Testing# 2022-02-21T17:00:14.349417: Epoch   4 Batch  168/468 accuracy = 0.422  test_loss = 1.76872\n",
      "#Testing# 2022-02-21T17:00:14.859057: Epoch   4 Batch  188/468 accuracy = 0.375  test_loss = 2.23295\n",
      "#Testing# 2022-02-21T17:00:15.328807: Epoch   4 Batch  208/468 accuracy = 0.469  test_loss = 1.93401\n",
      "#Testing# 2022-02-21T17:00:15.863477: Epoch   4 Batch  228/468 accuracy = 0.469  test_loss = 1.98247\n",
      "#Testing# 2022-02-21T17:00:16.341179: Epoch   4 Batch  248/468 accuracy = 0.469  test_loss = 1.72834\n",
      "#Testing# 2022-02-21T17:00:16.834861: Epoch   4 Batch  268/468 accuracy = 0.391  test_loss = 1.88254\n",
      "#Testing# 2022-02-21T17:00:17.317572: Epoch   4 Batch  288/468 accuracy = 0.453  test_loss = 1.99047\n",
      "#Testing# 2022-02-21T17:00:17.857131: Epoch   4 Batch  308/468 accuracy = 0.406  test_loss = 1.91787\n",
      "#Testing# 2022-02-21T17:00:18.354799: Epoch   4 Batch  328/468 accuracy = 0.391  test_loss = 1.95220\n",
      "#Testing# 2022-02-21T17:00:18.879394: Epoch   4 Batch  348/468 accuracy = 0.359  test_loss = 2.00131\n",
      "#Testing# 2022-02-21T17:00:19.468725: Epoch   4 Batch  368/468 accuracy = 0.516  test_loss = 1.69023\n",
      "#Testing# 2022-02-21T17:00:20.059145: Epoch   4 Batch  388/468 accuracy = 0.516  test_loss = 1.49694\n",
      "#Testing# 2022-02-21T17:00:20.618649: Epoch   4 Batch  408/468 accuracy = 0.469  test_loss = 1.90041\n",
      "#Testing# 2022-02-21T17:00:21.203088: Epoch   4 Batch  428/468 accuracy = 0.531  test_loss = 1.62867\n",
      "#Testing# 2022-02-21T17:00:21.771567: Epoch   4 Batch  448/468 accuracy = 0.453  test_loss = 1.83541\n",
      "best loss = 1.7655686269967983  acc = 0.46474358974358976\n",
      "Model saved in file: ./session\\best_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "net = network()\n",
    "total_batch_size = 64\n",
    "\n",
    "best_loss = 999\n",
    "\n",
    "for ii in range(epochs):\n",
    "    train_batches = get_batches(train_xs, train_ys, total_batch_size)\n",
    "    test_batches = get_batches(valid_xs, valid_ys, total_batch_size)\n",
    "    batch_num = (len(train_xs) // total_batch_size)\n",
    "    for batch_i in range(batch_num):\n",
    "        x, y = next(train_batches)\n",
    "        net.training(x, y, ii * (batch_num) + batch_i, ii, batch_i, batch_num)\n",
    "        \n",
    "    batch_num = (len(valid_xs) // total_batch_size)\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    for batch_i  in range(batch_num):\n",
    "        x, y = next(test_batches)\n",
    "        acc, loss = net.testing(x, y, ii * (batch_num) + batch_i, ii, batch_i, batch_num)\n",
    "        test_loss = test_loss + loss\n",
    "        test_acc = test_acc + acc\n",
    "        \n",
    "    test_acc = test_acc / batch_num\n",
    "    test_loss = test_loss / batch_num\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        print(\"best loss = {}  acc = {}\".format(best_loss, test_acc))\n",
    "        net.save()\n",
    "    else:\n",
    "        print(\"test loss = {}  acc = {}\".format(test_loss, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-4234775e7210>:13: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  conv1 = tf.layers.conv2d(self.x, filters=100, kernel_size=5, activation=tf.nn.relu)\n",
      "<ipython-input-44-4234775e7210>:15: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  self.conv1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2)\n",
      "<ipython-input-44-4234775e7210>:17: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  conv2 = tf.layers.conv2d(self.conv1, filters=150, kernel_size=3, activation=tf.nn.relu)\n",
      "<ipython-input-44-4234775e7210>:19: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  self.conv2 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
      "<ipython-input-44-4234775e7210>:22: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  conv3 = tf.layers.conv2d(self.conv2, filters=250, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
      "<ipython-input-44-4234775e7210>:24: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  self.conv3 = tf.layers.max_pooling2d(conv3, pool_size=2, strides=2)\n",
      "<ipython-input-44-4234775e7210>:27: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  self.fc0   = tf.layers.flatten(self.conv3)\n",
      "<ipython-input-44-4234775e7210>:30: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.fc1 = tf.layers.dense(self.fc0, units=512, activation=tf.nn.relu)\n",
      "<ipython-input-44-4234775e7210>:32: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.fc2 = tf.layers.dense(self.fc1, units=300, activation=tf.nn.relu)\n",
      "<ipython-input-44-4234775e7210>:34: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.logits = tf.layers.dense(self.fc2, units=n_class)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.shape =  (?, 28, 28, 100)\n",
      "conv2.shape =  (?, 12, 12, 150)\n",
      "max_pool conv2.shape =  (?, 6, 6, 150)\n",
      "conv3.shape =  (?, 6, 6, 250)\n",
      "max_pool conv3.shape =  (?, 3, 3, 250)\n",
      "fc0.shape =  (?, 2250)\n",
      "INFO:tensorflow:Restoring parameters from ./session\\best_model.ckpt\n",
      "Successfully loaded: ./session\\best_model.ckpt\n",
      "#Testing# 2022-02-21T17:00:52.797417: Epoch   0 Batch    0/156 accuracy = 0.531  test_loss = 1.76817\n",
      "#Testing# 2022-02-21T17:00:53.534451: Epoch   0 Batch   20/156 accuracy = 0.406  test_loss = 1.88650\n",
      "#Testing# 2022-02-21T17:00:54.134842: Epoch   0 Batch   40/156 accuracy = 0.469  test_loss = 1.87414\n",
      "#Testing# 2022-02-21T17:00:54.667419: Epoch   0 Batch   60/156 accuracy = 0.344  test_loss = 1.99367\n",
      "#Testing# 2022-02-21T17:00:55.197003: Epoch   0 Batch   80/156 accuracy = 0.469  test_loss = 1.90319\n",
      "#Testing# 2022-02-21T17:00:55.704646: Epoch   0 Batch  100/156 accuracy = 0.391  test_loss = 2.07405\n",
      "#Testing# 2022-02-21T17:00:56.215282: Epoch   0 Batch  120/156 accuracy = 0.469  test_loss = 2.03029\n",
      "#Testing# 2022-02-21T17:00:56.748854: Epoch   0 Batch  140/156 accuracy = 0.406  test_loss = 2.03870\n",
      "test loss = 1.9799896684976726  acc = 0.41065705128205127\n"
     ]
    }
   ],
   "source": [
    "net = network()\n",
    "\n",
    "test_batches = get_batches(test_set, y_test, total_batch_size)\n",
    "batch_num = (len(test_set) // total_batch_size)\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "for batch_i  in range(batch_num):\n",
    "    x, y = next(test_batches)\n",
    "    acc, loss = net.testing(x, y, 0 * (batch_num) + batch_i, 0, batch_i, batch_num)\n",
    "    test_loss = test_loss + loss\n",
    "    test_acc = test_acc + acc\n",
    "        \n",
    "test_acc = test_acc / batch_num\n",
    "test_loss = test_loss / batch_num\n",
    "print(\"test loss = {}  acc = {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下为TensorFlow2的网络设计和训练测试，数据集没有改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.python.ops import summary_ops_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络一开始的InputLayer可以使用这句代码代替： keras.layers.Reshape(target_shape=[32, 32, 1], input_shape=(32, 32,)),\n",
    "\n",
    "# 另外， InputLayer也可以省略，这样的话， 需要在Conv2D中指定输入的形状，网络前两层（InputLayer和Conv2D）可以用这句代码代替：\n",
    "# keras.layers.Conv2D (kernel_size = (5, 5), filters = 100, activation='relu', input_shape=(32, 32, 1)),\n",
    "\n",
    "# 注意， InputLayer并不是网络的第一层， 只是用来说明输入的尺寸\n",
    "def get_model():\n",
    "    model = keras.Sequential([\n",
    "            keras.layers.InputLayer(input_shape=(32, 32, 1)),\n",
    "            keras.layers.Conv2D (kernel_size = (5, 5), filters = 100, activation='relu'),\n",
    "            keras.layers.MaxPool2D(),\n",
    "            keras.layers.Conv2D (kernel_size = (3, 3), filters = 150, activation='relu'),\n",
    "            keras.layers.MaxPool2D(),\n",
    "            keras.layers.Conv2D (kernel_size = (3, 3), filters = 250, padding='same', activation='relu'),\n",
    "            keras.layers.MaxPool2D(),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(512, activation='relu'),\n",
    "            keras.layers.Dense(300, activation='relu'),\n",
    "            keras.layers.Dense(n_class, activation='softmax')\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 100)       2600      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 100)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 150)       135150    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 150)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 6, 6, 250)         337750    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 3, 3, 250)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2250)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               1152512   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 300)               153900    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 20)                6020      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,787,932\n",
      "Trainable params: 1,787,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"training_models/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True,\n",
    "    # Save weights, every 1-epochs.\n",
    "    period=1)\n",
    "\n",
    "callbacks = [\n",
    "    cp_callback,\n",
    "    # Interrupt training if `val_loss` stops improving for over 2 epochs\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    # Write TensorBoard logs to `./logs` directory\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "#warning无所谓的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120000 samples, validate on 30000 samples\n",
      "Epoch 1/5\n",
      "120000/120000 [==============================] - ETA: 0s - loss: 2.6732 - acc: 0.1840"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lxpperfect\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.42907, saving model to training_models\\cp-0001.ckpt\n",
      "120000/120000 [==============================] - 260s 2ms/sample - loss: 2.6732 - acc: 0.1840 - val_loss: 2.4291 - val_acc: 0.2573\n",
      "Epoch 2/5\n",
      "120000/120000 [==============================] - ETA: 0s - loss: 2.3263 - acc: 0.2907\n",
      "Epoch 2: val_loss improved from 2.42907 to 2.28520, saving model to training_models\\cp-0002.ckpt\n",
      "120000/120000 [==============================] - 246s 2ms/sample - loss: 2.3263 - acc: 0.2907 - val_loss: 2.2852 - val_acc: 0.2997\n",
      "Epoch 3/5\n",
      "120000/120000 [==============================] - ETA: 0s - loss: 2.1305 - acc: 0.3502\n",
      "Epoch 3: val_loss improved from 2.28520 to 2.08420, saving model to training_models\\cp-0003.ckpt\n",
      "120000/120000 [==============================] - 259s 2ms/sample - loss: 2.1305 - acc: 0.3502 - val_loss: 2.0842 - val_acc: 0.3594\n",
      "Epoch 4/5\n",
      "120000/120000 [==============================] - ETA: 0s - loss: 1.9737 - acc: 0.3979\n",
      "Epoch 4: val_loss improved from 2.08420 to 1.98391, saving model to training_models\\cp-0004.ckpt\n",
      "120000/120000 [==============================] - 228s 2ms/sample - loss: 1.9737 - acc: 0.3979 - val_loss: 1.9839 - val_acc: 0.3918\n",
      "Epoch 5/5\n",
      "120000/120000 [==============================] - ETA: 0s - loss: 1.8462 - acc: 0.4359\n",
      "Epoch 5: val_loss improved from 1.98391 to 1.88206, saving model to training_models\\cp-0005.ckpt\n",
      "120000/120000 [==============================] - 235s 2ms/sample - loss: 1.8462 - acc: 0.4359 - val_loss: 1.8821 - val_acc: 0.4216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230d1e548b0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "model.fit(train_xs, train_ys, epochs=5, batch_size=64, validation_data=(valid_xs, valid_ys), \n",
    "          callbacks=callbacks) # validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.3989\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_set, y_test)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以加载保存的参数继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120000 samples, validate on 30000 samples\n",
      "120000/120000 [==============================] - ETA: 0s - loss: 1.7265 - acc: 0.4693\n",
      "Epoch 1: val_loss improved from 1.88206 to 1.80626, saving model to training_models\\cp-0001.ckpt\n",
      "120000/120000 [==============================] - 232s 2ms/sample - loss: 1.7265 - acc: 0.4693 - val_loss: 1.8063 - val_acc: 0.4473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230d1719250>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.load_weights(latest)\n",
    "\n",
    "# The re-loaded model needs to be re-compiled.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_xs, train_ys, epochs=1, batch_size=64, validation_data=(valid_xs, valid_ys), \n",
    "          callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.4134\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_set, y_test)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
